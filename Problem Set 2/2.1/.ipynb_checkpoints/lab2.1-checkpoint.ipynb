{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COG403: Problem 1 of Problem Set 2: Semantic Representations\n",
    "\n",
    "### All 3 problems for Problem Set 2 Due 1 November 2018, 2:00pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we will look at three different approaches for generating word vectors. We will then evaluate these word vectors on their ability to match human similarity judgments on a set of noun-noun pairs.  Specifically, we will use a subset of SimLex-999$^1$ that contains animal nouns and has average similarity judgments on pairs of these obtained from human participants.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Write a function `generate_WN_vectors()` to generate feature vectors based on Word Net features$^2$. Generate these vectors and then save them in `data/word_net.vec` using the `write_vectors` function in `provided_functions.py`.\n",
    "\n",
    "Word net features are in the file `data/all_catf_norm_prob_lexicon_cs.all.txt`. The format is:\n",
    "    $$<word>:<part\\_of\\_speech>\\textrm{ }<feat1\\_name>:<feat1\\_val>,<feat2\\_name:feat2\\_val>,...$$\n",
    "\n",
    "Note that the feature names may contain spaces, which can be confusing, since the $<word>:<part\\_of\\_speech>$ is separated from the list of features by a space.\n",
    "\n",
    "Your approach for generating feature vectors for wordnet features should be a two-step process. First, collect all the features in the file `data/all_catf_norm_prob_lexicon_cs.all.txt` that occur with the words in `data/vocab.txt` (you can load the vocabulary using the `get_vocab` function defined in `provided_functions.py`). Then, generate feature vectors where each field in a vector corresponds to a feature, and the value in the field is the feature value. For example, if you found the features `['animal', 'mammal', 'carnivore']`, your vectors for *cat* and *rat* could be:\n",
    "\n",
    "* *cat*: `[0.18, 0.07, 0.02]`\n",
    "* *rat*: `[0.18, 0.07, 0]`\n",
    "\n",
    "Not all features will be listed for all animals. In these cases, set the field to be 0.0.\n",
    "\n",
    "Please print your vector for *cat*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "def load_vectors(path_to_file):\n",
    "    \"\"\"\n",
    "    path_to_file: str -- path to file to read vectors from\n",
    "    \n",
    "    Returns a dict of str to list of float, which maps words to their vectors. The\n",
    "    words and vectors are loaded from path_to_file.\n",
    "    \"\"\"\n",
    "    word_vector_dict = {}\n",
    "    with open(path_to_file) as f:\n",
    "        for line in f:\n",
    "            line = re.split(':[A-Z]+\\s', line) #This code did not work on '\\t'. I simply split with ':N ' or ':V ', etc.\n",
    "            word = line[0]\n",
    "            vector = np.array(line[1:])\n",
    "            for i in vector:\n",
    "                vector = vector[0].split(\",\") #This code did not work before because all feature vectors were concatenated into one string\n",
    "            \n",
    "            vector = np.array(vector)\n",
    "            word_vector_dict[word] = vector\n",
    "    return word_vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.       0.       0.\n",
      " 0.001427 0.       0.       0.       0.       0.       0.       0.\n",
      " 0.       0.      ]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import provided_functions as f \n",
    "vectors = load_vectors(\"data/all_catf_norm_prob_lexicon_cs.all.txt\")\n",
    "vocabulary = f.get_vocab(\"data/vocab.txt\")\n",
    "length_of_vocab = len(vocabulary)\n",
    "def generate_WN_vectors():\n",
    "    word_vector_dict = {}\n",
    "    for word in vectors:\n",
    "        word_vector_dict[word] = np.zeros((length_of_vocab,), dtype = float)\n",
    "\n",
    "    for word in vectors:\n",
    "        for fv in vectors[word]:\n",
    "            feature_vector = re.split(\"[#0-9]*:\", fv)\n",
    "            if feature_vector[0] in vocabulary:\n",
    "                word_vector_dict[word][vocabulary.index(feature_vector[0])] = feature_vector[1]\n",
    "    return word_vector_dict\n",
    "\n",
    "WN_vectors = generate_WN_vectors()\n",
    "f.write_vectors(\"data/word_net.vec\", WN_vectors)\n",
    "print(WN_vectors['cat'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Write a function `generate_SWOW_vectors()` to generate feature vectors based on Small World of Words$^3$ features. Generate these vectors and then save them in `data/small_world_of_words.vec` using the `write_vectors` function in `provided_functions.py`.\n",
    "\n",
    "The file `data/SWOW-EN.R100.csv` contains cues and responses for a free association task hosted online. In the task, the participant is shown a cue word and asked to respond by listing three response words.  (You can try this out yourself at http://www.smallworldofwords.org!)\n",
    "\n",
    "You should start by generating a zero-valued square matrix of cues by responses where the cues and responses are the words in `vocab.txt` (note: only save vectors for words in `vocab.txt` that occur as cues). Then read through the `data/SWOW-EN.R100.csv` file, and update the row in the matrix corresponding to each cue word, by adding 1 to the count of each response word that appears for that cue. For example, if your vocab (and thus the heading of your matrix columns) was `['cat', 'dog', 'goat', 'fluffy', 'kitten']`, and the first line in the data set had the cue *cat* and the responses [*dog*, *kitten*, *fluffy*], your updated vectors would look like this:\n",
    " * *cat*: [0, 1, 0, 1, 1]\n",
    " * *dog*: [0, 0, 0, 0, 0]\n",
    " * *goat*: [0, 0, 0, 0, 0]\n",
    " * *fluffy*: [0, 0, 0, 0, 0]\n",
    " * *kitten*: [0, 0, 0, 0, 0] \n",
    " \n",
    "After generating your cue-to-response vectors, set the diagonal entries of the matrix to be the highest value of each row; that is, the entry for the cue itself in the response vector of the cue should be set to the highest response count for that cue. This is done to make the vectors more appropriate for assessing similarity. We want the word *cat* to be the most similar to itself.\n",
    "\n",
    "For example, if we have the following vector for *cat* (assuming the same feature order as in the example above):\n",
    " * *cat*: [0, 30, 1, 20, 15],\n",
    " \n",
    "we would set the response count for *cat* to the max response count, 30, to get the following:\n",
    " * *cat*: [30, 30, 1, 20, 15]\n",
    "\n",
    "\n",
    "Please print your SWOW vector for *cat* as well as the value associated with *cat* in the cat vector. Make sure to label these two things clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SWOW vector for cat is: \n",
      "[ 0  0  4  0  0 10 61  6  0  0  1  0 12  0  0  0  0  0  1  0  0  1  1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 61  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      " and the value associated with cat in the cat vector is:  61\n"
     ]
    }
   ],
   "source": [
    "import provided_functions as f \n",
    "import csv\n",
    "import numpy as np\n",
    "vocabulary = f.get_vocab(\"data/vocab.txt\")\n",
    "length_vocab = len(vocabulary)\n",
    "\n",
    "def generate_SWOW_vectors():\n",
    "    cues_dict = {}\n",
    "    for i in vocabulary:\n",
    "        cues_dict[i] = np.zeros((length_vocab,), dtype=int)\n",
    "    \n",
    "    with open('data/SWOW-EN.R100.csv', 'rt') as f:\n",
    "        reader = csv.reader(f)\n",
    "        SWOW_entries = list(reader)\n",
    "    cues_response = []\n",
    "    for index in SWOW_entries:\n",
    "        if index[-4] in vocabulary:\n",
    "            cues_response.append([index[-4], index[-3],index[-2],index[-1]])\n",
    "    \n",
    "    for cr in cues_response:\n",
    "        for index in range(1, len(cr)):\n",
    "            if cr[index] in vocabulary:\n",
    "                cues_dict[cr[0]][vocabulary.index(cr[index])] += 1\n",
    "    for key, value in cues_dict.items():\n",
    "        value[vocabulary.index(key)] = np.amax(value)\n",
    "    return cues_dict\n",
    "\n",
    "            \n",
    "cues_dict = generate_SWOW_vectors()\n",
    "cat_vector = cues_dict[\"cat\"]\n",
    "cat_value = cues_dict[\"cat\"][vocabulary.index(\"cat\")]\n",
    "print (\"The SWOW vector for cat is: \\n\"\n",
    "       +str(cat_vector)\n",
    "       +\"\\n and the value associated with cat in the cat vector is: \",\n",
    "       str(cat_value))\n",
    "f.write_vectors(\"data/small_world_of_words.vec\", cues_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Write a function `evaluate_vectors` that takes a set of vectors and evaluates them using SimLex-999. Write your function according to the description below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "SIM_LEX_FILE = 'data/SimLex-999-Animals.txt'\n",
    "\n",
    "\n",
    "def evaluate_vectors(word_vector_dict):\n",
    "    \"\"\"\n",
    "    word_vector_dict: dict of str to list -- a dictionary mapping words to vectors.\n",
    "    \n",
    "    1. Iterate over the word pairs in the SimLex-999 Animal subset at `data/SimLex-999-Animals.txt`.\n",
    "       Compute the similarities between your word vectors for the word pairs using cosine similarity\n",
    "       (use `sklearn.metrics.pairwise.cosine_similarity`). Skip word pairs where one or both of the\n",
    "       vectors are missing from word_vector_dict.\n",
    "    2. Print the pearson correlation r, and the corresponding p value, between the SimLex-999\n",
    "       similarity scores and the cosine similarities for word pairs. (Use `scipy.stats.pearsonr`).\n",
    "       Please print \"Pearson r is: \", and \"with p value: \".\n",
    "    3. Use `matplotlib` to generate a plot of SimLex-999 similarity scores and cosine similarities\n",
    "       for word pairs.\n",
    "    \"\"\"\n",
    "    simlex_sim_scores = []\n",
    "    cos_sim_scores = np.array([])\n",
    "    with open (SIM_LEX_FILE, \"rt\") as f:\n",
    "        for line in f:\n",
    "            simlex = line.split(\"\\t\")\n",
    "            if ((simlex[0] in word_vector_dict) and (simlex[1] in word_vector_dict) ):\n",
    "                simlex_sim_scores.append(simlex[2].rstrip())\n",
    "                word1 = word_vector_dict[simlex[0]]\n",
    "                word2 = word_vector_dict[simlex[1]]\n",
    "                word1 = np.array(word1,dtype=float)\n",
    "                word2 = np.array(word2,dtype=float)\n",
    "                cos_sim = cosine_similarity([word1], [word2])\n",
    "                cos_sim_scores = np.concatenate((cos_sim_scores,cos_sim),axis=None)\n",
    "    simlex_sim_scores = (np.float_(simlex_sim_scores))\n",
    "    correlation_coefficient, p_value = pearsonr(simlex_sim_scores, cos_sim_scores)      \n",
    "    print (\"Pearson r is: \" + str(correlation_coefficient) + \" with p value: \" + str(p_value))\n",
    "    \n",
    "    plt.scatter(simlex_sim_scores, cos_sim_scores)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "Run `evaluate_vectors` from part c on the WordNet vectors generated in part a. Use `load_vectors` defined in `provided_functions.py` to load the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson r is: 0.36496849109725155 with p value: 0.07283242716946996\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import provided_functions as f \n",
    "WN_vec = f.load_vectors(\"data/word_net.vec\")\n",
    "evaluate_vectors(WN_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "\n",
    "Run `evaluate_vectors` from part c on the Small World of Words vectors generated in part b. Use `load_vectors` defined in `provided_vectors.py` to load the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson r is: 0.30020703341442356 with p value: 0.08451655032349062\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAE9JJREFUeJzt3X1sXfddx/H3d27KvEcz6qHFSUhAWbZqAWWz2kIk2CNJB2pC2ViLBgNVRGPrNsYUlMA0UAE1WhBsSGWiG2UPjFali7xoK2RoKRqa1irOjJa1nSFkW2On0OzBBTGPpuHLH7Zb27n2vfZ9OL6/+35JUXzPPffc7/2d64/P+Z3fOScyE0lSuZ5RdQGSpPYy6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFu6yqN77iiity8+bNVb29JHWlkydPfiszB1fymsqCfvPmzYyOjlb19pLUlSLimyt9jV03klQ4g16SCmfQS1Lh6gZ9RNwREY9FxFeXeD4i4s8j4nREfCUiXt76MiVJq9XIFv1Hgd3LPH8tsHX23z7gQ82XJUlqlbpBn5lfAL6zzCx7gI/njPuBgYh4UasKlCQ1pxV99EPA2XmPJ2anXSIi9kXEaESMnj9/vgVvLUmqpxVBHzWm1bw/YWbenpnDmTk8OLii8f6SpFVqxQlTE8DGeY83AOdasNzijIxNcvjYOOemplk/0M/+XdvYu6Pmzo8ktUwrtuiPAr86O/rmGuDxzHy0BcstysjYJAePnGJyapoEJqemOXjkFCNjk1WXJqlwjQyvvBP4ErAtIiYi4qaIeGtEvHV2lnuBM8Bp4MPA29pWbRc7fGyc6QsXF0ybvnCRw8fGK6pIUq+o23WTmTfWeT6Bt7esokKdm5pe0XRJahXPjO2Q9QP9K5ouSa1i0HfI/l3b6F/Xt2Ba/7o+9u/aVlFFknpFZZcp7jVzo2scdSOp0wz6Dtq7Y8hglzrE4cxPM+glFWduOPPcSLe54cxAT4a9ffSSiuNw5oUMeknFcTjzQga9pOI4nHkhg15ScRzOvJAHYyUVx+HMCxn0kirXjqGQDmd+mkEvqVIOhWw/++glVcqhkO1n0EuqlEMh28+gl1Qph0K2n0EvqVIOhWw/D8ZKqpRDIdvPoJdUOYdCtpddN5JUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqXENBHxG7I2I8Ik5HxIEaz2+KiPsiYiwivhIRr299qVLvGBmbZOeh42w58Fl2HjrOyNhk1SWpi9UN+ojoA24DrgWuBG6MiCsXzfZe4O7M3AHcAPxFqwuVesXcPVQnp6ZJnr6HqmGv1Wpki/4q4HRmnsnMJ4C7gD2L5kngebM/Px8417oSpd7iPVTVao1cj34IODvv8QRw9aJ5/gD4XES8A3g28NqWVCf1IO+hqlZrZIs+akzLRY9vBD6amRuA1wOfiIhLlh0R+yJiNCJGz58/v/JqpR7gPVTVao0E/QSwcd7jDVzaNXMTcDdAZn4JeCZwxeIFZebtmTmcmcODg4Orq1gqnPdQVas1EvQngK0RsSUiLmfmYOvRRfM8ArwGICJeykzQu8kurcLeHUPcev12hgb6CWBooJ9br9/urfa0anX76DPzyYi4GTgG9AF3ZOaDEXELMJqZR4H3AB+OiHcz063za5m5uHtHUoO8h6paqaGbg2fmvcC9i6a9b97PDwE7W1uaJKkVPDNWkgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFa6hoI+I3RExHhGnI+LAEvP8UkQ8FBEPRsTftrZMSdJqXVZvhojoA24DXgdMACci4mhmPjRvnq3AQWBnZn43Il7YroIlSSvTyBb9VcDpzDyTmU8AdwF7Fs3zG8BtmfldgMx8rLVlSpJWq5GgHwLOzns8MTttvhcDL46IL0bE/RGxu1UFSpKaU7frBoga07LGcrYCrwQ2AP8cES/LzKkFC4rYB+wD2LRp04qLlSStXCNb9BPAxnmPNwDnaszz6cy8kJlfB8aZCf4FMvP2zBzOzOHBwcHV1ixJWoFGgv4EsDUitkTE5cANwNFF84wArwKIiCuY6co508pCJUmrU7frJjOfjIibgWNAH3BHZj4YEbcAo5l5dPa5n42Ih4CLwP7M/HY7C5fUHUbGJjl8bJxzU9OsH+hn/65t7N2x+DCf2ikyF3e3d8bw8HCOjo5W8t6SGtdMUI+MTXLwyCmmL1x8alr/uj5uvX67Yb9KEXEyM4dX8hrPjJW0pLmgnpyaJoHJqWkOHjnFyNhkQ68/fGx8QcgDTF+4yOFj422oVktpZNSNCuZutZazXFA38j05NzW9oulqD4O+hy3erZ7bWgMM+8Ks9g96s0G9fqCfyRrzrh/ob+j1ag27bnqYu9W9oZnul6UCudGg3r9rG/3r+hZM61/Xx/5d2xp6vVrDoO9h7lb3hmb+oDcb1Ht3DHHr9dsZGugngKGB/p49EDsyNsnOQ8fZcuCz7Dx0vOHjHK1g100Pc7e6NzTzB30ukJs5jrN3x1BPBvt8VXeTGvQ9bP+ubTWHvrlbXZZm/6Ab1M1r9qB2s+y66WHuVveG5bpfquxO6CVVd5O6Rd/j3For31LdL4Cjrjqk6m5Sg17qAbX+oO88dLzS7oReUnU3qUG/DE8mqs12KUPV3Qm9pBUHtZth0C+h6qPka5XtUo6quxN6TZXdpB6MXYInE9Vmu5TDk5l6h1v0S3C3tjbbpRxVdyeocwz6JbhbW5vtUhZHXfUGu26W4G5tbbaL1H3col+Cu7W12S5S9/EOU5LURbzDlCTpEnbdtIAnEKnd/I6pGQZ9kzyBSO3md0zNsuumSZ5ApHbzO6ZmGfRN8gQitZvfMTXLoG9Ss/fUlOrxO6ZmGfRN8gQitZvfMTXLg7FN8gQitZvfMTXLE6YkqYt4wpQk6RIGvSQVzqCXpMIZ9JJUuIaCPiJ2R8R4RJyOiAPLzPeGiMiIWNGBAklS+9QN+ojoA24DrgWuBG6MiCtrzPdc4J3AA60uUpK0eo1s0V8FnM7MM5n5BHAXsKfGfH8IvB/4fgvrkyQ1qZGgHwLOzns8MTvtKRGxA9iYmZ9ZbkERsS8iRiNi9Pz58ysuVpK0co0EfdSY9tRZVhHxDODPgPfUW1Bm3p6Zw5k5PDg42HiVkqRVayToJ4CN8x5vAM7Ne/xc4GXAP0XEN4BrgKMekJWktaGRoD8BbI2ILRFxOXADcHTuycx8PDOvyMzNmbkZuB+4LjO9voEkrQF1gz4znwRuBo4BDwN3Z+aDEXFLRFzX7gIlSc1p6OqVmXkvcO+iae9bYt5XNl+WJKlVPDNWkgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhGrqVoKS1YWRsksPHxjk3Nc36gX7279rG3h1DVZelNc6gl7rEyNgkB4+cYvrCRQAmp6Y5eOQUgGGvZdl1I3WJw8fGnwr5OdMXLnL42HhFFalbGPRSlzg3Nb2i6dIcu27Us7qtv3v9QD+TNUJ9/UB/W9+329pJl3KLXj1prr97cmqa5On+7pGxyapLW9L+XdvoX9e3YFr/uj7279rWtvfsxnbSpQz6LjcyNsnOQ8fZcuCz7Dx03F/ABnVjf/feHUPcev12hgb6CWBooJ9br9/e1q3rbmwnXcqumy7mKIzV69b+7r07hjq6bru1nbSQW/RdrJVbW722Z7BUv3a7+7u7je1UBoO+i7Vqa6sX+2Gr6O/uRrZTGQz6Ltaqra1e7Ietor+7G9lOZbCPvovt37VtQR89rG5rq1f7YTvd392tbKfu19AWfUTsjojxiDgdEQdqPP/bEfFQRHwlIj4fET/S+lK1WKu2tuyHlcpWd4s+IvqA24DXARPAiYg4mpkPzZttDBjOzO9FxG8C7wfe1I6CtdBqtrYWnwDzqpcM8qmTk03vGUhamxrZor8KOJ2ZZzLzCeAuYM/8GTLzvsz83uzD+4ENrS1TrVLrwOunTk7yi68Ysh9WKlQjffRDwNl5jyeAq5eZ/ybg75spSu2z1IHX+752ni8eeHVFVUlqp0aCPmpMy5ozRrwZGAZ+Zonn9wH7ADZt2tRgiWqlXj3wKvWyRrpuJoCN8x5vAM4tnikiXgv8HnBdZv5vrQVl5u2ZOZyZw4ODg6upV03ywKvUexoJ+hPA1ojYEhGXAzcAR+fPEBE7gL9kJuQfa32ZahVPgJF6T92um8x8MiJuBo4BfcAdmflgRNwCjGbmUeAw8Bzg7yIC4JHMvK6NdWuV5g6wetlZqXdEZs3u9rYbHh7O0dHRSt5bkrpVRJzMzOGVvMZLIEhS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCtfIzcG1ho2MTXq3KEnLMui72MjYJAePnGL6wkUAJqemOXjkFIBhL+kpdt10scPHxp8K+TnTFy5y+Nh4RRVJWosM+i52bmp6RdMl9SaDvoutH+hf0XRJvcmg72L7d22jf13fgmn96/rYv2tbRRVJWos8GNvF5g64OupG0nIM+i63d8eQwS5pWXbdSFLhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUuIbG0UfEbuCDQB/wkcw8tOj5HwA+DrwC+Dbwpsz8RmtLnbHUZXnfO3KKOx84y8VM+iK48eqN/NHe7W1/36q1o652f9bVLr8T62CtrudOqPfZu6H9e3n9LScyc/kZIvqAfwVeB0wAJ4AbM/OhefO8DfjxzHxrRNwA/EJmvmm55Q4PD+fo6OiKil18WV6YOeX/5Zuezxf//TuXzP/maza1JOyXet9br99e6ZeoHXW1+7OudvmdWAdrdT13Qr3P3g3t3yvrLyJOZubwSl7TSNfNVcDpzDyTmU8AdwF7Fs2zB/jY7M/3AK+JiFhJIY1Y6rK8tUIe4M4Hzrb1fau+HHA76mr3Z13t8juxDtbqeu6Eep+9G9q/l9dfPY0E/RAwPzEnZqfVnCcznwQeB35o8YIiYl9EjEbE6Pnz51dc7Eovv3uxzt5Ks+9b9eWA21FXuz/rapffiXWwVtdzJ9T77N3Q/r28/uppJOhrbZkvTtBG5iEzb8/M4cwcHhwcbKS+BVZ6+d2+Fu1UrNXLAbejrnZ/1tUuvxPrYK2u506o99m7of17ef3V00jQTwAb5z3eAJxbap6IuAx4PlC7P6UJS12Wd+ePvaDm/DdevbHm9Fa9b9WXA25HXe3+rKtdfifWwVpdz51Q77N3Q/v38vqrp5FRNyeArRGxBZgEbgB+edE8R4G3AF8C3gAcz3pHeVdhucvytnPUzVq9HHA76mr3Z13t8juxDtbqeu6Eep+9G9q/l9dfPXVH3QBExOuBDzAzvPKOzPzjiLgFGM3MoxHxTOATwA5mtuRvyMwzyy1zNaNuJKnXrWbUTUPj6DPzXuDeRdPeN+/n7wNvXMkbS5I6wzNjJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqXEMnTLXljSPOA9+s8dQVwLc6XM5aZDvYBnNshxm2w4xtmfnclbygoROm2iEza17VLCJGV3rWV4lsB9tgju0ww3aYERErvqSAXTeSVDiDXpIKtxaD/vaqC1gjbAfbYI7tMMN2mLHidqjsYKwkqTPW4ha9JKmF1kzQR8TuiBiPiNMRcaDqeqoQERsj4r6IeDgiHoyId1VdU5Uioi8ixiLiM1XXUpWIGIiIeyLia7Pfi5+suqYqRMS7Z38nvhoRd87eA6N4EXFHRDwWEV+dN+0FEfGPEfFvs///YL3lrImgj4g+4DbgWuBK4MaIuLLaqirxJPCezHwpcA3w9h5thznvAh6uuoiKfRD4h8x8CfAT9GB7RMQQ8E5gODNfxswNkG6otqqO+Siwe9G0A8DnM3Mr8PnZx8taE0EPXAWczswzmfkEcBewp+KaOi4zH83ML8/+/N/M/FL35H3QImID8HPAR6qupSoR8Tzgp4G/AsjMJzJzqtqqKnMZ0D97T+pncel9q4uUmV/g0vtv7wE+Nvvzx4C99ZazVoJ+CDg77/EEPRpwcyJiMzO3Znyg2koq8wHgd4D/q7qQCv0ocB7469kurI9ExLOrLqrTMnMS+BPgEeBR4PHM/Fy1VVXqhzPzUZjZOAReWO8FayXoo8a0nh0OFBHPAT4F/FZm/lfV9XRaRPw88Fhmnqy6lopdBrwc+FBm7gD+hwZ200sz2we9B9gCrAeeHRFvrraq7rJWgn4C2Djv8QZ6ZNdssYhYx0zIfzIzj1RdT0V2AtdFxDeY6cZ7dUT8TbUlVWICmMjMub26e5gJ/l7zWuDrmXk+My8AR4CfqrimKv1nRLwIYPb/x+q9YK0E/Qlga0RsiYjLmTnQcrTimjouIoKZ/tiHM/NPq66nKpl5MDM3ZOZmZr4LxzOz57bgMvM/gLMRsW120muAhyosqSqPANdExLNmf0deQw8elJ7nKPCW2Z/fAny63gsqu6jZfJn5ZETcDBxj5oj6HZn5YMVlVWEn8CvAqYj4l9lpv5uZ91ZYk6r1DuCTsxtAZ4Bfr7iejsvMByLiHuDLzIxMG6NHzpKNiDuBVwJXRMQE8PvAIeDuiLiJmT+Cb6y7HM+MlaSyrZWuG0lSmxj0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQV7v8B1AFrGYm63tcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import provided_functions as f\n",
    "SWOW_vec = f.load_vectors('data/small_world_of_words.vec')\n",
    "evaluate_vectors(SWOW_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)\n",
    "\n",
    "Run `evaluate_vectors` from part c on word2vec vectors stored in `data/word2vec.vec`. Use `load_vectors` defined in `provided_functions.py` to load the vectors.\n",
    "\n",
    "These vectors are a subset of vectors from a 2017 paper by Fares$^4$ with:\n",
    " * dimension: 300\n",
    " * window: 5\n",
    " * corpus: Gigaword 5th Edition\n",
    " * vocab size: 261794\n",
    " * algorithm: Gensim Continuous Skipgram\n",
    " * lemmatization: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson r is: 0.4893719058835222 with p value: 0.0033110382715266892\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import provided_functions as f\n",
    "W2_vec = f.load_vectors('data/word2vec.vec')\n",
    "evaluate_vectors(W2_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g)\n",
    "\n",
    " * 1. Compare the performance of the three vector spaces referring to both the results of your Pearson correlations and the scatterplots.\n",
    "\n",
    " * 2. For each features space, examine some example similarity comparisons and the corresponding word vectors. From your inspection, what do you think is responsible for the different performance of the vector spaces in this task? Show example noun-noun pairs that support your hypothesis about why each of the vector spaces performs well or does not perform well.  Explain how these examples support your hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. <br/>\n",
    "The performance goes as follows:\n",
    "\n",
    "* For d) available data points for these vector spaces from word_net is limited. Most, if not all elements within the vector is 0.0. Therefore, a pearson correlation is highly sensitive to the vectors whose cosine similarity scores are not zero. We cannot for sure say that there is a correlation at the 95% confidence level.\n",
    "* For e), unlike d), has many more elements in the vector available to compare cosine similarity. Generally, the scatter plot appears as though there is a correlation. Given by the values of pearsonr, we can see this is not the case at the 95% confidence level as the p-value = 0.08451655032349062. However, I suspect outliers and zero-datapoints heavily affect the correlation coefficient and p-value. Such datapoints skew the data, and in removing them we may better see a p-value and correlation coefficient more reflective of the data.\n",
    "* For f), unlike d) and e), has many datapoints available, and the data generally consistent with a correlation (e.g. no obvious outliers, no zero-points). At the 95% confidence level, we may conclude that data in word2vec has a significant correlation with simlex-999 data points, and the correlation coefficient is 0.4893719058835222.\n",
    "<br\\>\n",
    "* 2.\n",
    "Likewise from the previous answer, such a disprepancy in the vector space in WordNet has a large affect on the performance in this task. Some vectors, such as for bee and ant have some data point that matches because they both refer to the same weight in the limitation. Vectors such as dog and cat, however, do not have any datapoints that correspond, therefore their cosine similarity is effectively 0. \n",
    "In another case, outliers and zero heavily influence the performance of the vector spaces. For example, dog and cat have a .97 cosine similarity score, the highest of any Noun-Noun pair in this data. Yet, simlex-999 scores it a 2. And likewise from WordNet vector, zero-points of cosine similarity also affect the performance. Such datapoints skew the regression of the data and ultimately hinders the performance of the model.\n",
    "\n",
    "In the W2_vec vector space, the datapoints and their cosine similarity scores are generally consistent with the rest of the data, i.e. no outliers or zero-scores. The data available is ultimately more than the WordNet and SWOW vectors. It regresses into a positive correlation at the p-value of 0.0033110382715266892, less than 0.05 level that allows us to come to the conclusion that such correlation of simlex-999 and cosine similarity scores of W2_vec is statistically significant at the 95% confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet Nouns example\n",
      "Bee:\n",
      "['0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.002152' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.038736' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0\\n']\n",
      "Ant:\n",
      "['0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.002152' '0.0' '0.0' '0.038736' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0\\n']\n",
      "Dog:\n",
      "['0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.001135' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0\\n']\n",
      "Cat:\n",
      "['0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.001427' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0\\n']\n",
      "SWOW Nouns example\n",
      "Dog:\n",
      "['0' '0' '0' '0' '0' '4' '51' '17' '0' '0' '0' '0' '1' '0' '0' '0' '2' '0'\n",
      " '0' '0' '0' '1' '1' '1' '0' '0' '0' '1' '0' '2' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '51' '0' '0' '0' '0' '0' '0' '0' '0' '0\\n']\n",
      "Cat:\n",
      "['0' '0' '4' '0' '0' '10' '61' '6' '0' '0' '1' '0' '12' '0' '0' '0' '0'\n",
      " '0' '1' '0' '0' '1' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '61' '0' '0' '0' '0' '0' '0' '0' '0' '0\\n']\n",
      "Word2Vec Nouns example\n",
      "Dog:\n",
      "['-0.187956' '-0.576374' '-0.099378' '0.381505' '-0.034920' '0.662993'\n",
      " '0.403795' '-0.504661' '0.271683' '0.664742' '0.572801' '0.391702'\n",
      " '-0.175632' '0.334175' '-0.418716' '0.880376' '-0.776999' '-0.236448'\n",
      " '-0.437179' '-0.402898' '0.605225' '0.206453' '0.649173' '-0.020535'\n",
      " '0.195228' '0.435480' '-0.356695' '-0.297003' '0.753823' '0.381901'\n",
      " '-0.023558' '0.116041' '0.165225' '-1.328727' '-0.235878' '-0.186595'\n",
      " '0.233233' '0.502537' '-0.169190' '0.450573' '-0.293062' '-0.262067'\n",
      " '0.070613' '0.016534' '0.536001' '-0.063618' '-0.563297' '0.950493'\n",
      " '-0.198283' '0.173649' '0.235086' '-0.458623' '-0.309464' '-0.461100'\n",
      " '0.097848' '-0.115201' '-0.183937' '0.220529' '-0.441323' '0.074265'\n",
      " '0.154289' '0.072733' '0.183437' '-0.199054' '0.440639' '-0.171841'\n",
      " '0.650229' '-0.539144' '-0.423035' '-0.048725' '-0.322653' '-0.270446'\n",
      " '0.567587' '0.308086' '-0.202252' '0.208237' '0.038470' '-0.492895'\n",
      " '0.432774' '-0.614658' '-0.416508' '0.006910' '-0.396625' '-0.437460'\n",
      " '-0.623182' '0.809147' '-0.282259' '-0.001973' '0.149887' '-0.037521'\n",
      " '0.245675' '0.608327' '0.015966' '-0.414597' '-0.489306' '-0.546594'\n",
      " '0.172616' '0.101673' '0.091192' '0.573523' '0.078577' '0.175116'\n",
      " '-0.189040' '0.153342' '-0.165493' '-0.429415' '0.107025' '-0.106172'\n",
      " '-0.084381' '-0.032291' '-0.075368' '0.289532' '0.260989' '0.369629'\n",
      " '-0.210340' '0.486072' '0.263833' '0.536831' '-0.769980' '0.198002'\n",
      " '0.108243' '0.545785' '0.227985' '-0.036271' '-0.443213' '0.218040'\n",
      " '0.202533' '0.358055' '0.710656' '0.392202' '0.489327' '0.531588'\n",
      " '-0.772029' '0.633901' '0.395475' '0.334173' '0.493336' '0.105454'\n",
      " '0.065573' '0.250915' '-0.188365' '0.393228' '-0.068369' '0.051130'\n",
      " '0.150347' '0.357629' '0.011406' '0.276524' '-0.227215' '-0.125641'\n",
      " '-0.036376' '-0.698272' '0.538157' '0.191639' '0.239891' '0.277397'\n",
      " '0.384187' '0.491824' '0.137515' '1.030566' '0.305299' '-0.153995'\n",
      " '0.178249' '-0.662220' '0.147328' '-0.151256' '-0.317225' '-0.447727'\n",
      " '0.589411' '-0.259636' '0.013431' '0.445777' '-0.182228' '0.354243'\n",
      " '0.043861' '0.088199' '0.497814' '0.023080' '-0.555684' '-0.516628'\n",
      " '0.337557' '0.311753' '-0.640268' '-0.563423' '-0.103659' '-0.443959'\n",
      " '-0.060210' '-0.066899' '0.103711' '-0.123077' '0.652969' '0.434432'\n",
      " '0.412643' '0.560435' '0.348187' '-0.056629' '0.175321' '-0.162380'\n",
      " '-0.952616' '0.343147' '-0.602855' '0.147280' '0.286469' '-0.575492'\n",
      " '0.468590' '-0.717664' '0.029476' '-0.452041' '0.459889' '0.227545'\n",
      " '0.204830' '-0.204705' '-0.226857' '0.695111' '0.678019' '-0.311193'\n",
      " '-0.367845' '0.141134' '-0.656574' '0.150177' '-0.239690' '-0.216816'\n",
      " '0.331253' '-0.764553' '0.560034' '-0.289125' '0.725344' '0.189100'\n",
      " '-0.169837' '-1.180877' '-0.219115' '0.625226' '-0.234790' '-0.068018'\n",
      " '-0.051659' '-0.380299' '0.371037' '0.274763' '-0.381641' '0.517303'\n",
      " '-1.136787' '1.142170' '-0.167943' '-0.760565' '0.282648' '0.048736'\n",
      " '0.247873' '0.849435' '0.765334' '0.131228' '0.520798' '0.867567'\n",
      " '-0.508811' '-0.332433' '-0.505514' '-0.085914' '0.220988' '-0.446214'\n",
      " '-0.252348' '-0.090061' '0.054504' '-0.427272' '0.056685' '-0.249915'\n",
      " '0.455869' '-1.264609' '0.413469' '0.354768' '0.412730' '-0.792996'\n",
      " '-0.056604' '0.331234' '0.270079' '0.031969' '0.310917' '0.095629'\n",
      " '-0.986801' '-0.460804' '-0.073847' '-0.670419' '-0.043895' '-0.384747'\n",
      " '-0.002839' '-0.173417' '-0.300731' '-0.191447' '-0.552234' '0.271966'\n",
      " '-0.111534' '-0.219022' '0.006802' '-0.274981' '0.007225' '0.145600'\n",
      " '0.202521' '0.280714' '0.256343' '-0.467481' '0.475499' '-0.218778\\n']\n",
      "Cat:\n",
      "['-0.030510' '-0.219258' '0.208356' '0.605209' '0.031824' '0.539372'\n",
      " '0.831034' '-0.498476' '0.348149' '0.482086' '0.973080' '-0.051308'\n",
      " '-0.424477' '0.370969' '0.082189' '1.075413' '-0.186794' '-0.713830'\n",
      " '-0.257695' '-0.599751' '0.522187' '0.198099' '0.143058' '0.065377'\n",
      " '0.076064' '-0.095881' '-0.227032' '-0.310105' '0.200591' '0.268745'\n",
      " '-0.113465' '0.232583' '0.265311' '-0.416211' '0.253842' '0.070479'\n",
      " '0.483618' '0.387476' '0.051666' '0.232553' '-0.459484' '-0.173974'\n",
      " '-0.025405' '0.155713' '0.402326' '0.124108' '0.190185' '0.823629'\n",
      " '0.537462' '0.314799' '0.380077' '-0.284195' '-0.357232' '-0.206095'\n",
      " '0.360582' '0.022357' '-0.379062' '-0.103901' '-0.230969' '0.138418'\n",
      " '-0.297476' '-0.138301' '0.249721' '-0.287777' '0.218165' '-0.718574'\n",
      " '1.049481' '-0.322890' '-0.074914' '-0.155123' '0.086249' '0.373635'\n",
      " '0.004990' '0.409668' '0.112650' '-0.003822' '-0.126497' '-0.817465'\n",
      " '0.387146' '-0.590701' '-0.191143' '0.265951' '-0.271256' '0.104922'\n",
      " '-0.266301' '0.577893' '-0.164821' '0.213611' '-0.000028' '0.144063'\n",
      " '0.620689' '0.342102' '0.297409' '0.044080' '-0.500889' '-0.791786'\n",
      " '0.042862' '0.057525' '-0.466203' '1.106704' '-0.052683' '0.629375'\n",
      " '0.017494' '0.041805' '-0.160238' '-0.244396' '0.240456' '-0.222708'\n",
      " '0.261083' '0.205089' '-0.461600' '0.360231' '0.232016' '-0.290220'\n",
      " '0.157973' '-0.055750' '0.086785' '0.069648' '-0.513269' '0.031227'\n",
      " '-0.356844' '-0.099294' '0.826368' '-0.415003' '-0.192453' '0.212814'\n",
      " '-0.520959' '0.480930' '0.649174' '0.790247' '0.697772' '0.343829'\n",
      " '-0.020461' '0.193717' '0.406501' '0.875910' '0.160783' '0.431040'\n",
      " '-0.293468' '0.193610' '-0.496300' '0.376313' '-0.115967' '0.067954'\n",
      " '0.026157' '0.206465' '-0.300431' '0.111386' '-0.135749' '-0.486364'\n",
      " '0.222474' '-0.773868' '0.824136' '0.207038' '0.326333' '0.672248'\n",
      " '0.501042' '-0.304307' '0.375674' '0.242500' '-0.089511' '-0.567449'\n",
      " '0.263215' '-0.338777' '-0.237821' '-0.257867' '-0.024144' '0.049573'\n",
      " '0.737534' '-0.160888' '0.069165' '0.010884' '0.223232' '0.447325'\n",
      " '0.173929' '0.634444' '0.191132' '0.290159' '-0.109394' '-0.394099'\n",
      " '0.413886' '0.016523' '-0.432952' '-0.133243' '-0.065065' '-0.175450'\n",
      " '-0.110246' '0.063655' '0.483706' '0.219550' '0.724847' '0.260669'\n",
      " '0.545532' '-0.105634' '0.527781' '-0.140979' '0.335768' '0.262027'\n",
      " '-0.729536' '-0.064289' '-0.549974' '-0.001336' '0.670889' '-0.201478'\n",
      " '0.071257' '-0.247750' '-0.185479' '-0.003455' '0.757716' '0.321316'\n",
      " '-0.203066' '-0.351589' '0.606485' '0.611296' '0.274556' '0.015465'\n",
      " '-0.092076' '-0.132292' '-0.345324' '-0.009838' '0.082957' '-0.227774'\n",
      " '-0.028869' '-0.383367' '-0.041273' '-0.791424' '0.520078' '-0.015747'\n",
      " '0.228250' '-0.399343' '-0.563174' '0.360613' '0.059697' '-0.226602'\n",
      " '0.100663' '-0.257228' '0.635062' '0.754916' '-0.038834' '0.334358'\n",
      " '-1.224287' '0.524054' '-0.190877' '-0.372278' '0.079715' '0.182402'\n",
      " '0.037008' '0.856984' '0.876060' '0.186854' '0.108800' '-0.016716'\n",
      " '-0.511060' '-0.093950' '-0.065025' '-0.079481' '0.328206' '-0.296119'\n",
      " '-0.061437' '-0.061705' '-0.010320' '-0.213708' '-0.159285' '-0.137934'\n",
      " '0.023403' '-0.718247' '0.226487' '0.459930' '-0.263499' '-0.280784'\n",
      " '-0.290351' '0.415764' '-0.021996' '-0.502458' '0.392624' '-0.200775'\n",
      " '-0.704582' '-0.238976' '-0.343274' '-0.461438' '-0.071727' '-0.066764'\n",
      " '0.070002' '-0.272428' '-0.064685' '0.058221' '-0.585899' '0.664546'\n",
      " '-0.068528' '-0.627903' '0.382968' '-0.936954' '-0.289952' '0.080468'\n",
      " '0.422128' '0.216063' '0.019948' '-0.278022' '0.529659' '-0.488361\\n']\n"
     ]
    }
   ],
   "source": [
    "WN_vec = f.load_vectors(\"data/word_net.vec\")\n",
    "print (\"WordNet Nouns example\")\n",
    "print (\"Bee:\")\n",
    "print (WN_vec[\"bee\"])\n",
    "print (\"Ant:\")\n",
    "print (WN_vec[\"ant\"])\n",
    "print (\"Dog:\")\n",
    "print (WN_vec[\"dog\"])\n",
    "print (\"Cat:\")\n",
    "print (WN_vec[\"cat\"])\n",
    "SWOW_vec = f.load_vectors('data/small_world_of_words.vec')\n",
    "print (\"SWOW Nouns example\")\n",
    "print (\"Dog:\")\n",
    "print (SWOW_vec[\"dog\"])\n",
    "print (\"Cat:\")\n",
    "print (SWOW_vec[\"cat\"])\n",
    "W2_vec = f.load_vectors('data/word2vec.vec')\n",
    "print (\"Word2Vec Nouns example\")\n",
    "print (\"Dog:\")\n",
    "print (W2_vec[\"dog\"])\n",
    "print (\"Cat:\")\n",
    "print (W2_vec[\"cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "$^1$Hill, F., Reichart, R., & Korhonen, A. (2015). SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation. Computational Linguistics, 41(4), 665-695. doi:10.1162/coli_a_00237\n",
    "\n",
    "$^2$Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D., & Miller, K. J. (1990). Introduction to WordNet: An On-line Lexical Database*. International Journal of Lexicography, 3(4), 235-244. doi:10.1093/ijl/3.4.235\n",
    "\n",
    "$^3$Data is from http://www.smallworldofwords.org.\n",
    "\n",
    "$^4$ Fares, Murhaf; Kutuzov, Andrei; Oepen, Stephan & Velldal, Erik (2017). Word vectors, reuse, and replicability: Towards a community repository of large-text resources, In Jörg Tiedemann (ed.), Proceedings of the 21st Nordic Conference on Computational Linguistics, NoDaLiDa, 22-24 May 2017. Linköping University Electronic Press. ISBN 978-91-7685-601-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
