{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## COG403: Problem 2 of Problem Set 1: Betas and Homophones\n",
    "\n",
    "### All 3 problems for Problem Set 1 Due 4 October 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are a child learning English. You are building up your vocabulary, but you are struggling with homophones (words that mean different things but sound the same). In particular, you are working on learning the difference between *for* and *four*, both of which are phonetically [fɔɹ]. (This is IPA. For more information, see https://en.wikipedia.org/wiki/International_Phonetic_Alphabet).\n",
    "\n",
    "You assume that there is a parameter, $\\theta$, that controls how often [fɔɹ] conveys each meaning (*for* and *four*). This assumption can be formalized as the graphical model shown below. Each $X_i$ represents a sentence that uses a word that sounds like [fɔɹ]. $X_i$ takes on value 1 if [fɔɹ] in sentence $i$ means *for* and 0 if it means *four*. The assumption underlying this graphical model is that there is some unobserved value of $\\theta$ and that for each sentence, a biased coin is flipped to determine whether a word that sounds like [fɔɹ] will mean *for* (with probability $\\theta$) or mean *four* (with probability $1- \\theta$ ). You would like to learn the value of $\\theta$ by observing sentences that contain words that are pronounced [fɔɹ].\n",
    "\n",
    "We will model learning [fɔɹ] based on corpus data.\n",
    "\n",
    "![betas graphical model](https://notebooks.azure.com/juliawatson/libraries/q2-betas-and-homophones/raw/grapical_model_betas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) \n",
    "\n",
    "Write a function to load in the Brent Ratner child-directed speech corpus and return a dictionary mapping each word type to its frequency in the corpus. This corpus is stored in `data/brent-ratner-corpus.txt` in the library for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_word_counts(path_to_file):\n",
    "    \"\"\"\n",
    "    path_to_file: string -- the path to the corpus file\n",
    "    \n",
    "    Return a dictionary mapping each unique type in the corpus at path_to_file\n",
    "    to the number of times it occurs in the corpus. Make sure to convert words\n",
    "    to all lowercase to get unique types.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    file = open(path_to_file, \"r\")\n",
    "    for line in file:\n",
    "        for word in line.split():\n",
    "            word = word.lower()\n",
    "            counter[word]+= 1\n",
    "    return counter\n",
    "            \n",
    "\n",
    "corpus_counts = compute_word_counts('data/brent-ratner-corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "The estimate of $\\theta$ that maximizes your posterior distribution is called the MAP estimate (MAP stands for maximum a posteriori), which we refer to as $\\hat{\\theta}$. \n",
    "\n",
    "You can model your posterior distribution as $Beta(a_1 + k_1, a_2 + k_2)$, where $a_1$ and $a_2$ are from your prior distribution, and $k_1$ is the number of observed sentences with [fɔɹ] that meant *for*, and $k_2$ is the number of observed sentences with [fɔɹ] that meant *four*. \n",
    "\n",
    "When you model the posterior distribution this way, then you can compute $\\hat{\\theta}$ using the formula $\\frac{a_1+k_1−1}{(a_1+k_1−1)+(a_2+k_2−1)}$ . \n",
    "\n",
    "Use this formula to compute $\\hat{\\theta}$ after observing child-directed utterances below (taken from the Brent Ratner Corpus$^1$ from CHILDES$^2$):\n",
    "1. This is food **for** the dragon.\n",
    "2. One block, two blocks, three blocks, **four** blocks.\n",
    "3. Thank you **for** the phone.\n",
    "4. What do you want to get **for** your birthday?\n",
    "\n",
    "Assume an initial prior distrubution of $Beta(1, 1)$. You may do this by hand or write Python code. If you choose to do it by hand, be sure to show your work.\n",
    "\n",
    "Note: for more information on the Beta distribution see the tutorial from week 3:\n",
    "https://betastutorial-juliawatson.notebooks.azure.com/j/notebooks/betas.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume prior distribution Beta(1, 1), i.e. $a_1 = 1 \\bigwedge a_2 = 1$<br/>\n",
    "Observing child-directed utterances yield 3 instances where 'for' is the intended reference and 1 instance where 'four' is the intended reference. <br/>\n",
    "Then $k_1 = 3$ and $k_2 = 1$<br/>\n",
    "$\\hat{\\theta}$ calculates to $\\frac{1+3−1}{(1+3−1)+(1+1−1)} = 0.75$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "You have two friends, Jack and Jill, who are also trying to learn the meaning of [fɔɹ]. Their learning biases are different from yours. Jack has a prior distribution of $Beta(10,10)$ and Jill has a prior distribution of $Beta(100,100)$. After observing the utterances from part b, what are the parameters of their posterior distributions? What are their MAP estimates of $\\theta$? You may do this by hand or write Python code. If you choose to do it by hand, be sure to show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their parameters of their posterior distributions is given by $(a_1 + k_1, a_2 + k_2)$.\n",
    "<br/>\n",
    "Where $a_1$ and $a_2$ are the parameters of the prior distribution and $k_1$ and $k_2$ are given by the observing utterances from part b. In this case, $k_1 = 3$ and $k_2 = 1$.\n",
    "For Jack, it is $(10 + 3, 10 + 1)$.\n",
    "For Jill, it is $(100 + 3, 100 + 1)$.\n",
    "\n",
    "Their MAP estimates of $\\theta$, i.e. $\\hat{\\theta}$ is given by $\\frac{a_1+k_1−1}{(a_1+k_1−1)+(a_2+k_2−1)}$\n",
    "<br/>\n",
    "For Jack, it is $\\hat{\\theta} = \\frac{10+3−1}{(10+3−1)+(10+1−1)} = 0.54545454545$ <br/>\n",
    "For Jill, it is $\\hat{\\theta} = \\frac{100+3−1}{(100+3−1)+(100+1−1)} = 0.50495049505$ <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "Write a function that uses the word frequencies computed in part a above to compute the probability of word given a list of its homophones. This probability will serve as the \"true\" $\\theta$ value -- the value that the child is seeking to learn from the sample of data they're exposed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_theta(corpus_counts, word1, homophone_list):\n",
    "    \"\"\"\n",
    "    corpus_counts: dict of str->int, mapping words to their frequencies\n",
    "    word1: str\n",
    "    homophone_list: list of words that sound the same as word1. word1 must be in homophone_list.\n",
    "    \n",
    "    Return the probability of word1 given that a word from homophone_list occurred.\n",
    "    \"\"\"\n",
    "    prob_num = (corpus_counts[word1])\n",
    "    prob_denom = 0\n",
    "    for word in homophone_list:\n",
    "        prob_denom += corpus_counts[word]\n",
    "    probability = (float(prob_num)/float(prob_denom))\n",
    "    return probability\n",
    "true_theta = compute_theta(corpus_counts, \"for\", [\"for\", \"four\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "\n",
    "Compute the squared error of each of the MAP estimates (for you, Jack, and Jill) based on the true $\\theta$ computed in part d. Who had the lowest squared error: you, Jack, or Jill? You may do this by hand or write Python code. If you choose to do it by hand, be sure to show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let squared error of the MAP estimates be\n",
    "$$E = (\\delta)^{2} = (\\theta - \\hat{\\theta})^{2}$$ \n",
    "For me, given by the calculations of my MAP estimate given in part b),\n",
    "my squared error of my MAP estimate is\n",
    "$$ E_{Me} = (\\delta)^{2}_{Me} = (0.9074074074074074 - 0.75)^{2} = 0.0247770919$$\n",
    "For Jack, given by the calculations of the MAP estimate given in c),\n",
    "their squared error of their MAP estimate is\n",
    "$$ E_{Jack} = (\\delta)^{2}_{Jack} = (0.9074074074074074 - 0.54545454545)^{2} = 0.13100987427$$\n",
    "For Jill, their's is\n",
    "$$ E_{Jill} = (\\delta)^{2}_{Jill} = (0.9074074074074074 - 0.50495049505)^{2} = 0.1619715663$$\n",
    "\n",
    "Therefore, I have the lowest squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)\n",
    "\n",
    "Write a function `generate_corpus` that creates a random corpus that you and your friends might encounter based on the theta value computed in part d. For any integer $n$, your command should return a 1-by-n vector consisting of ones (uses of [fɔɹ] that mean *for*) and zeros (uses of [fɔɹ] that mean *four*), where 1 occurs approximately $\\theta$ fraction of the time and 0 occurs approximately (1 - $\\theta$) fraction of the time. (Hint: using `numpy.random.rand(n)` will give you a vector of $n$ random numbers uniformly sampled from $[0,1)$. How can you use this to generate a list of ones and zeros where 1 occurs $\\theta$ fraction of the time?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "def generate_corpus (true_theta, n):\n",
    "    return (random.rand(n) < true_theta).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g)\n",
    "\n",
    "Write a function `learn` to simulate a learner. Your function should take in a number $n$, the parameters $a_1$ and $a_2$ of the Beta prior distribution, and the true $\\theta$ value. It should first generate a random corpus of length $n$ (using `generate_corpus` from f), then use this corpus together with the prior to find the parameters of the posterior distribution, and finally use those parameters to compute the MAP estimate $\\hat{\\theta}$ and the squared error of this estimate. Your function should return the MAP estimate as well as the squared error.\n",
    "\n",
    "Test out this function by calling it using:\n",
    " * `a_1 = a_2 = 1`\n",
    " * `true_theta` = the true theta value for *for* (computed in part d)\n",
    " * `n = 100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.94, 0.0010622770919067159)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def learn(a1, a2, n, true_theta):\n",
    "    \"\"\"\n",
    "    a1: int -- parameter for prior Beta distribution\n",
    "    a2: int -- parameter for prior Beta distribution\n",
    "    n: int -- number of samples to use to update Beta distribution\n",
    "    true_theta: float -- the theta value we want to model. We use it to generate a corpus.\n",
    "\n",
    "    Return MAP theta value and squared error for Beta(a1, a2) after seeing n examples\n",
    "    of a word that sounds like \"for\" used to mean *for* and *four*. The examples are\n",
    "    generated randomly, with \"for\" meaning *for* true_theta fraction of the time and\n",
    "    meaning *four* (1 - true_theta) fraction of the time.\n",
    "    \"\"\"\n",
    "    corpus = generate_corpus(true_theta, n)\n",
    "    k1 = 0\n",
    "    k2 = 0\n",
    "    for k in corpus:\n",
    "        if k == 1:\n",
    "            k1 += 1\n",
    "        elif k == 0:\n",
    "            k2 += 1\n",
    "    theta_num = (a1 + k1 - 1)\n",
    "    theta_denom = ((a1 + k1 - 1) + (a2 + k2 - 1))\n",
    "    theta = ((float(theta_num))/(float(theta_denom)))\n",
    "    squared_error = (true_theta - theta)**2\n",
    "    return theta, squared_error\n",
    "        \n",
    "    \n",
    "\n",
    "learn(1, 1, 100, true_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (h)\n",
    "\n",
    "Run an experiment to see which initial beta distribution produces the best results across multiple corpora: Write a function `evaluate_learners` that runs your simulation `learn` 1,000 times for each of five corpus sizes ($n$=1, 2, 3, 4, and 5) and for each of the three learners (you, Jack, and Jill). For each corpus size and each learner, compute the average value of $\\hat{\\theta}$ and the average squared error across the 1,000 trials, and print a summary. (To clarify: your script should run `learn` a total of 15,000 times.)\n",
    "\n",
    "In your print statements, make sure to round any numbers to four decimal places, so that they are easily interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_learners(word1, word2):\n",
    "    \"\"\"\n",
    "    Run trials for You, Jack, and Jill for different numbers of samples (1-5). Print a report of the\n",
    "    average theta and average MSE for each set of trials.\n",
    "\n",
    "    Based on the directions in the handout, assume that:\n",
    "      - You have an initial distribution of Beta(1, 1)\n",
    "      - Jack has an initial distribution of Beta(10, 10)\n",
    "      - Jill has an initial distribution of Beta(100, 100)\n",
    "\n",
    "    \"\"\"\n",
    "    homophone_list = [word1, word2]\n",
    "    local_true_theta = compute_theta(corpus_counts, word1, homophone_list)\n",
    "    learners = [\"Josh\", \"Jack\", \"Jill\"]\n",
    "    for learner in learners:\n",
    "        print(\"For \" + learner + \":\")\n",
    "        if learner == \"Josh\":\n",
    "            a1 = 1\n",
    "            a2 = 1\n",
    "        elif learner == \"Jack\":\n",
    "            a1 = 10\n",
    "            a2 = 10\n",
    "        elif learner == \"Jill\":\n",
    "            a1 = 100\n",
    "            a2 = 100\n",
    "        for size in range(1, 6):\n",
    "            theta_total = 0.0\n",
    "            mse_total = 0.0\n",
    "            for run in range(1000):\n",
    "                data = (learn(a1, a2, size, local_true_theta))\n",
    "                theta_total += data[0]\n",
    "                mse_total += data[1]\n",
    "            theta_average = theta_total/1000.0\n",
    "            mse_average = mse_total/1000.0\n",
    "            print(\"with size \" + str(size) + \n",
    "                   \":\"+ \"\\nAverage theta is \" + str(round(theta_average, 4)) + \n",
    "                   \"\\nAverage MSE is \" + str(round(mse_average, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i)\n",
    "\n",
    "Run `evaluate_learners` on *for* and *four*. For each of Me, Jack, and Jill, explain the impact of the number of samples on the mean theta and the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Josh:\n",
      "with size 1:\n",
      "Average theta is 0.921\n",
      "Average MSE is 0.0729\n",
      "with size 2:\n",
      "Average theta is 0.898\n",
      "Average MSE is 0.0452\n",
      "with size 3:\n",
      "Average theta is 0.909\n",
      "Average MSE is 0.0281\n",
      "with size 4:\n",
      "Average theta is 0.9135\n",
      "Average MSE is 0.0196\n",
      "with size 5:\n",
      "Average theta is 0.9034\n",
      "Average MSE is 0.018\n",
      "For Jack:\n",
      "with size 1:\n",
      "Average theta is 0.5211\n",
      "Average MSE is 0.1495\n",
      "with size 2:\n",
      "Average theta is 0.5411\n",
      "Average MSE is 0.1346\n",
      "with size 3:\n",
      "Average theta is 0.5577\n",
      "Average MSE is 0.1229\n",
      "with size 4:\n",
      "Average theta is 0.5745\n",
      "Average MSE is 0.1115\n",
      "with size 5:\n",
      "Average theta is 0.5889\n",
      "Average MSE is 0.1022\n",
      "For Jill:\n",
      "with size 1:\n",
      "Average theta is 0.5021\n",
      "Average MSE is 0.1642\n",
      "with size 2:\n",
      "Average theta is 0.5041\n",
      "Average MSE is 0.1626\n",
      "with size 3:\n",
      "Average theta is 0.5061\n",
      "Average MSE is 0.1611\n",
      "with size 4:\n",
      "Average theta is 0.508\n",
      "Average MSE is 0.1596\n",
      "with size 5:\n",
      "Average theta is 0.51\n",
      "Average MSE is 0.158\n"
     ]
    }
   ],
   "source": [
    "evaluate_learners(\"for\", \"four\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For \"Josh\", as the sample size increases, the mean theta approaches true_theta most of the three rapidly due to his initial lack of exposure. Therefore, the average mean squared error starts off high, but decreases most rapidly as well. \n",
    "\n",
    "For \"Jack\", as the sample size increases, the mean theta approaches true_theta slower than Josh. This is because Jack already has some exposure with these two homophones, so their measure of their MAP estimate will change, but not as quickly as if they had no exposure. Therefore, their average mean squared error starts smaller, but the sample size is not large enough to make significant decreases.\n",
    "\n",
    "For \"Jill\", as the sample size increases, the mean theta approaches true_theta so slowly that the impact is almost non-existent. Jill has a lot of exposure with these two homophones, so their measure of their MAP estimate will change, but only if the sample size increases above by merely 5. Therefore, the sample size has very small impact on the mean squared error, and will take a much larger sample size consistent with true_theta to actually dramatically reduce error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (j)\n",
    "\n",
    "Run `evaluate_learners` on the homophones *too* and *two*, which are both phonetically [tu]. Which learner does best (you, Jack, or Jill)? Is this different from the results for the homophone pair (*for*, *four*)? Explain why or why not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Josh:\n",
      "with size 1:\n",
      "Average theta is 0.407\n",
      "Average MSE is 0.2414\n",
      "with size 2:\n",
      "Average theta is 0.401\n",
      "Average MSE is 0.1207\n",
      "with size 3:\n",
      "Average theta is 0.4157\n",
      "Average MSE is 0.0772\n",
      "with size 4:\n",
      "Average theta is 0.4125\n",
      "Average MSE is 0.058\n",
      "with size 5:\n",
      "Average theta is 0.4126\n",
      "Average MSE is 0.047\n",
      "For Jack:\n",
      "with size 1:\n",
      "Average theta is 0.4938\n",
      "Average MSE is 0.0087\n",
      "with size 2:\n",
      "Average theta is 0.4901\n",
      "Average MSE is 0.0086\n",
      "with size 3:\n",
      "Average theta is 0.4861\n",
      "Average MSE is 0.0084\n",
      "with size 4:\n",
      "Average theta is 0.4825\n",
      "Average MSE is 0.0081\n",
      "with size 5:\n",
      "Average theta is 0.4803\n",
      "Average MSE is 0.0082\n",
      "For Jill:\n",
      "with size 1:\n",
      "Average theta is 0.4995\n",
      "Average MSE is 0.0091\n",
      "with size 2:\n",
      "Average theta is 0.499\n",
      "Average MSE is 0.009\n",
      "with size 3:\n",
      "Average theta is 0.4983\n",
      "Average MSE is 0.0089\n",
      "with size 4:\n",
      "Average theta is 0.4982\n",
      "Average MSE is 0.0089\n",
      "with size 5:\n",
      "Average theta is 0.4979\n",
      "Average MSE is 0.0088\n"
     ]
    }
   ],
   "source": [
    "evaluate_learners(\"too\", \"two\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learner that does \"best\" is arguably Jack. Jack's average mean squared error is smaller than Josh's and Jill's across all sample sizes, and especially when it increases.\n",
    "\n",
    "This is a different result from previously, and this is because the true_theta calculated for \\tu\\ involves two criterion that affects the mean squared error of each person.\n",
    "\n",
    "1. closeness to MAP estimate\n",
    "2. elasticity of learning\n",
    "\n",
    "For Josh, while elasticity of learning is high, the MAP estimate is widely variable, which makes the distance of his MAP estimate change dramatically, and inconsistently, therefore his mean squared error is the largest of all learners.\n",
    "\n",
    "For Jill, while the MAP estimate is steady and consistent, the inelasticity of learning due to large exposure inhibits Jill's MAP estimate to approach true_theta. \n",
    "\n",
    "For Jack, their MAP estimates are relatively consistent, yet doesn't sacrifice elasticity of learning. This combination allows the MAP estimate to approach true_theta faster than Jill, and more consistently than Josh.\n",
    "\n",
    "The reason why these results are so dramatically different than \\fɔɹ\\ is because the true_theta for \\fɔɹ\\ is further away from the learner's MAP estimate than \\tu\\, encouraging elastic learning over consistency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (k)\n",
    "\n",
    "The human language data we give our model is very limited (just counts). What additional information do you think children use to learn the difference between homophones like *for* and *four*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional aid may include explicit direction of differentiation of homophones.\n",
    "\n",
    "Rather than passive learning, where a child merely counts a word's occurance in a specific context, an explicit differentiation with their definitions may help them map the features to a particular homophone. For example, teaching a child that \"four\" is a number in a specific context where numbers are involved, and that \"for\" is a preposition used in context where a relations of nouns are involved will allow further distinction between the two word's meaning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "$^1$ Brent, M.R. and T.A. Cartwright. 1996. Distributional regularity and phonotactic constraints are useful for segmentation. Cognition 61: 93-125.\n",
    "\n",
    "$^2$ B.MacWhinney and C. Snow. 1985. The child language data exchange system. Journal of Child Language, 12:271-296."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
