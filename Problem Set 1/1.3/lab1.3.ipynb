{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## COG403: Problem 3 of Problem Set 1: Surprisal\n",
    "\n",
    "### All 3 problems for Problem Set 1 Due 4 October 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: To edit this notebook, click \"clone\", and it will prompt you to create your own copy of this library. Make sure to create a private library, so others can't see your answers. For more details, see the How-To Guide for Azure Notebooks posted on Quercus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arpa in /Users/JoshGelua/anaconda3/lib/python3.5/site-packages (0.1.0b2)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "P(there|hi) = 0.0025\n",
      "P(there|hi) = 0.0025\n",
      "log_p(there|hi) = -2.5984\n",
      "'hi' in demo_model.vocabulary() = 1.0000\n",
      "P(there|well hi) != 0.0025\n"
     ]
    }
   ],
   "source": [
    "# This installs a Python library called arpa, which is used for loading arpa format language files.\n",
    "# For more information, see: https://pypi.org/project/arpa/\n",
    "# Run this cell before running other cells in this notebook that use the arpa library.\n",
    "\n",
    "!pip install arpa\n",
    "import arpa\n",
    "\n",
    "# Here are some examples of how to use arpa:\n",
    "\n",
    "demo_model = arpa.loadf(\"data/bnc_10k_with_smoothing.arpa\")[0]\n",
    "\n",
    "# demo_model.p will give you the conditional probability of the last item given what came before\n",
    "print(\"P(there|hi) = {:.4f}\".format(demo_model.p([\"hi\", \"there\"])))\n",
    "\n",
    "# You can pass a list of terms or a string (arpa will automatically split on spaces).\n",
    "print(\"P(there|hi) = {:.4f}\".format(demo_model.p(\"hi there\")))\n",
    "\n",
    "# demo_model.log_p will give you log probabilities\n",
    "print(\"log_p(there|hi) = {:.4f}\".format(demo_model.log_p([\"hi\", \"there\"])))\n",
    "\n",
    "# demo_model.vocabulary() will return the vocabulary of the language model\n",
    "print(\"'hi' in demo_model.vocabulary() = {:.4f}\".format(\"hi\" in demo_model.vocabulary()))\n",
    "\n",
    "# For this problem set, the arpa models are bigram models. Arpa models will not raise an error\n",
    "# when passed trigrams, but the probabilities are not correct. It actually returns the bigram\n",
    "# probability of the last two tokens in the input.\n",
    "print(\"P(there|well hi) != {:.4f}\".format(demo_model.p([\"well\", \"hi\", \"there\"])))  # same output as demo_model.p([hi\", \"there\"])\n",
    "\n",
    "# Note that all input to the arpa model should be lowercase. The following will raise an error:\n",
    "# demo_model.p([\"Hi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Surprisal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smith and Levy$^1$ explored the shape of the relationship between a word’s predictability in context and how long, on average, a comprehender spends on reading that word. In particular they were interested in the hypothesis that a word’s average reading time might be linear in the surprisal of the word (Hale, 2001; Levy, 2008):\n",
    "\n",
    "$$\\textrm{surprisal}(w_i|\\textrm{Context}) = \\log\\frac{1}{P(w_i|\\textrm{Context})} \\quad\\quad (1)$$\n",
    "\n",
    "or in some other function of the word’s conditional probability. In this assignment, you will investigate this question yourself, using one of the datasets analyzed by Smith and Levy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Write a function to compute the surprisal of a bigram using a bigram arpa model. In the case of out of vocabulary (OOV) items (words that don't occur in our training corpus), return None.\n",
    "\n",
    "Compute the surprisal of the word \"toast\" in the bigram \"like toast\" using the bigram arpa model trained on the British National Corpus (BNC)$^2$ with smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.721252353445184"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arpa\n",
    "import numpy as np\n",
    "\n",
    "model_with_smoothing = arpa.loadf(\"data/bnc_10k_with_smoothing.arpa\")[0]\n",
    "vocab = set(model_with_smoothing.vocabulary())\n",
    "\n",
    "\n",
    "def compute_surprisal(arpa_model, bigram):\n",
    "    \"\"\"\n",
    "    Return the surprisal of the second word in bigram based on bigram probabilities. Return None when\n",
    "    one of the words in bigram is an OOV word.\n",
    "    \"\"\"\n",
    "    bigram = bigram.split(\" \")\n",
    "    if bigram[1] in vocab:\n",
    "        surprisal = np.log((1/(arpa_model.p(bigram))))\n",
    "        return surprisal\n",
    "    else:\n",
    "        return None\n",
    "compute_surprisal(model_with_smoothing, \"like toast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) \n",
    "\n",
    "Compute the surprisal of the word \"toast\" in the bigram \"like toast\" using a bigram model trained on the BNC without smoothing. Is the surprisal of word \"toast\" without smoothing higher or lower than that of the model with smoothing? Explain why you think this is.\n",
    "\n",
    "If it's lower, come up with an example that would have higher surprisal without smoothing than with smoothing. If it's higher, come up with an example of a bigram that would have a lower surprisal without smoothing than with smoothing.  Explain how you came up with that example.\n",
    "\n",
    "Hint: investigate the arpa file to see which bigrams occur in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239.98596423156536"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_path = \"data/bnc_10k_no_smoothing.arpa\"\n",
    "model_without_smoothing = arpa.loadf(corpus_path)[0]\n",
    "compute_surprisal(model_without_smoothing, \"like toast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The surprisal of the word \"toast\" without smoothing is higher than that of the model with smoothing.\n",
    "\n",
    "This is due to the absence of noise in the bigram model with smoothing to allow important patterns to show. Since the model with smoothing already has important patterns emphasized, a bigram \"like toast\" provides less information given that the words following \"like\" is 'toast-like', i.e. words like \"toast\" are common with words after \"like\". The model without smoothing is absent of the bigram \"like toast\", therefore calculating a higher surprisal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471905439975"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_surprisal(model_without_smoothing, \"nitrogen oxide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3455075817125475"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_surprisal(model_with_smoothing, \"nitrogen oxide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In attempting to find the opposite example of \"like toast\", \"nitrogen oxide\" is contained in the bigram model without smoothing, and a low surprisal is due to \"nitrogen\" having a limited domain of following words. So the probability that \"oxide\" follows \"nitrogen\" is higher than \"toast\" following \"like\". In the bigram, \"like\" has a vastly wider range of following words, so without smoothing, the probability that \"toast\" follows \"like\" is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Surprisal and Reading Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Write a function to load in the Brown Corpus$^3$ annotated with reading times.\n",
    "\n",
    "In `data/brown_reading_times.csv`, you can access the Smith and Levy$^1$ self-paced reading dataset. This dataset derives from each subject in the experiment reading a number of several-hundred-word passages selected from the\n",
    "Brown corpus. This dataset is presented in tabular format, with one reading-time measurement per row and with the following columns:\n",
    "\n",
    " * The **word** that was read;\n",
    " * A **code** that uniquely identifies the word/context pair;\n",
    " * The identifier for the **subject** in the experiment from which the reading-time measurement was taken;\n",
    " * **text_id**, which is the identifier for the text from the Brown corpus that was being read;\n",
    " * **text_pos**, which is the word number in the text selection that was being read;\n",
    " * **word_in_exp** is the word number in the experiment for the particular subject;\n",
    " * The **time** in milliseconds that the word in question was visible on-screen during the subject’s reading. (Remember, in self-paced reading this is the time elapsed between the subject pressing a button/key to reveal the word, and the subject pressing the button/key again to mask that word and reveal the next word.)\n",
    "\n",
    "Take a look at the function `get_sentences_and_reading_times` in `provided_functions.py`. Run this function on `data/brown_reading_times.csv`.\n",
    "\n",
    "Write a function `get_reading_times_and_bigrams` to generate a list of bigrams and reading times for the second word in the bigram. Run it on the sentences and reading times returned by `get_sentences_and_reading_times`. Use the BNC at `data/bnc_10k.txt` as your train corpus (this is used to compute bigram counts for thresholding as described in the docstring for `get_reading_times_and_bigrams` below).\n",
    "\n",
    "Print the first 10 bigrams you generate as well as their associated reading times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten bigrams: \n",
      "[('have', 'a'), ('would', 'be'), ('me', 'i'), ('that', 'i'), ('able', 'to'), ('come', 'to'), ('about', 'the'), ('through', 'the'), ('that', 'he'), ('but', 'the')]\n",
      "and their respective reading times: \n",
      "[336.15, 283.28, 286.62, 143.27, 241.72, 261.62, 180.61, 587.6, 222.87, 326.77]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('have', 'a'),\n",
       "  ('would', 'be'),\n",
       "  ('me', 'i'),\n",
       "  ('that', 'i'),\n",
       "  ('able', 'to'),\n",
       "  ('come', 'to'),\n",
       "  ('about', 'the'),\n",
       "  ('through', 'the'),\n",
       "  ('that', 'he'),\n",
       "  ('but', 'the'),\n",
       "  ('it', 'the'),\n",
       "  ('to', 'the'),\n",
       "  ('of', 'this'),\n",
       "  ('on', 'to'),\n",
       "  ('and', 'the'),\n",
       "  ('with', 'the'),\n",
       "  ('to', 'be'),\n",
       "  ('be', 'in'),\n",
       "  ('say', 'that'),\n",
       "  ('on', 'a'),\n",
       "  ('to', 'a'),\n",
       "  ('you', 'have'),\n",
       "  ('have', 'the'),\n",
       "  ('when', 'i'),\n",
       "  ('or', 'a'),\n",
       "  ('in', 'the'),\n",
       "  ('to', 'make'),\n",
       "  ('in', 'his'),\n",
       "  ('a', 'new'),\n",
       "  ('i', 'think'),\n",
       "  ('of', 'a'),\n",
       "  ('he', 'have'),\n",
       "  ('make', 'a'),\n",
       "  ('and', 'we'),\n",
       "  ('but', 'it'),\n",
       "  ('but', 'i'),\n",
       "  ('who', 'have'),\n",
       "  ('the', 'same'),\n",
       "  ('will', 'be'),\n",
       "  ('over', 'the'),\n",
       "  ('the', 'first'),\n",
       "  ('to', 'have'),\n",
       "  ('by', 'a'),\n",
       "  ('is', 'a'),\n",
       "  ('for', 'a'),\n",
       "  ('be', 'a'),\n",
       "  ('on', 'the'),\n",
       "  ('by', 'the'),\n",
       "  ('may', 'be'),\n",
       "  ('and', 'in'),\n",
       "  ('and', 'that'),\n",
       "  ('that', 'a'),\n",
       "  ('and', 'an'),\n",
       "  ('with', 'a'),\n",
       "  ('of', 'their'),\n",
       "  ('of', 'his'),\n",
       "  ('from', 'a'),\n",
       "  ('the', 'other'),\n",
       "  ('and', 'it'),\n",
       "  ('look', 'at'),\n",
       "  ('or', 'the'),\n",
       "  ('part', 'of'),\n",
       "  ('in', 'this'),\n",
       "  ('do', 'you'),\n",
       "  ('as', 'the'),\n",
       "  ('in', 'and'),\n",
       "  ('but', 'a'),\n",
       "  ('as', 'a'),\n",
       "  ('of', 'the'),\n",
       "  ('of', 'it'),\n",
       "  ('want', 'to'),\n",
       "  ('from', 'the'),\n",
       "  ('if', 'i'),\n",
       "  ('in', 'an'),\n",
       "  ('when', 'the'),\n",
       "  ('as', 'an'),\n",
       "  ('in', 'a'),\n",
       "  ('the', 'time'),\n",
       "  ('at', 'the'),\n",
       "  ('and', 'he'),\n",
       "  ('for', 'the'),\n",
       "  ('the', 'new'),\n",
       "  ('to', 'an'),\n",
       "  ('and', 'a'),\n",
       "  ('and', 'i'),\n",
       "  ('up', 'a'),\n",
       "  ('use', 'a'),\n",
       "  ('that', 'the'),\n",
       "  ('where', 'the'),\n",
       "  ('go', 'to'),\n",
       "  ('as', 'i'),\n",
       "  ('number', 'of'),\n",
       "  ('one', 'of'),\n",
       "  ('to', 'do'),\n",
       "  ('into', 'the'),\n",
       "  ('you', 'can'),\n",
       "  ('up', 'the'),\n",
       "  ('on', 'as'),\n",
       "  ('all', 'the'),\n",
       "  ('her', 'a'),\n",
       "  ('have', 'to'),\n",
       "  ('it', 'to'),\n",
       "  ('be', 'the')],\n",
       " [336.15,\n",
       "  283.28,\n",
       "  286.62,\n",
       "  143.27,\n",
       "  241.72,\n",
       "  261.62,\n",
       "  180.61,\n",
       "  587.6,\n",
       "  222.87,\n",
       "  326.77,\n",
       "  368.12,\n",
       "  183.39,\n",
       "  358.61,\n",
       "  153.5,\n",
       "  444.75,\n",
       "  294.03,\n",
       "  155.95,\n",
       "  373.61,\n",
       "  188.27,\n",
       "  250.69,\n",
       "  328.62,\n",
       "  457.88,\n",
       "  293.91,\n",
       "  247.26,\n",
       "  298.77,\n",
       "  198.95,\n",
       "  244.46,\n",
       "  199.12,\n",
       "  304.38,\n",
       "  306.64,\n",
       "  268.48,\n",
       "  358.56,\n",
       "  267.37,\n",
       "  391.22,\n",
       "  281.91,\n",
       "  155.6,\n",
       "  327.67,\n",
       "  183.4,\n",
       "  321.12,\n",
       "  228.64,\n",
       "  287.24,\n",
       "  237.76,\n",
       "  298.01,\n",
       "  178.34,\n",
       "  320.87,\n",
       "  234.91,\n",
       "  228.21,\n",
       "  132.16,\n",
       "  390.52,\n",
       "  188.52,\n",
       "  253.5,\n",
       "  290.34,\n",
       "  205.87,\n",
       "  206.27,\n",
       "  226.37,\n",
       "  288.5,\n",
       "  179.61,\n",
       "  263.62,\n",
       "  255.32,\n",
       "  181.46,\n",
       "  471.45,\n",
       "  374.82,\n",
       "  844.0,\n",
       "  232.82,\n",
       "  292.26,\n",
       "  228.78,\n",
       "  814.7,\n",
       "  277.36,\n",
       "  282.73,\n",
       "  501.07,\n",
       "  158.88,\n",
       "  222.95,\n",
       "  193.57,\n",
       "  381.91,\n",
       "  351.36,\n",
       "  242.42,\n",
       "  540.37,\n",
       "  177.72,\n",
       "  423.28,\n",
       "  182.85,\n",
       "  188.12,\n",
       "  326.89,\n",
       "  374.85,\n",
       "  355.56,\n",
       "  358.46,\n",
       "  384.04,\n",
       "  166.8,\n",
       "  265.13,\n",
       "  130.76,\n",
       "  250.03,\n",
       "  264.31,\n",
       "  215.54,\n",
       "  285.3,\n",
       "  247.38,\n",
       "  153.33,\n",
       "  304.91,\n",
       "  328.81,\n",
       "  228.34,\n",
       "  193.11,\n",
       "  396.13,\n",
       "  161.88,\n",
       "  320.83,\n",
       "  286.9])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as random\n",
    "from provided_functions import read_sentences_and_reading_times\n",
    "\n",
    "sentences, reading_times = read_sentences_and_reading_times(\"data/brown_reading_times.csv\")\n",
    "def get_reading_times_and_bigrams(\n",
    "        sentences, reading_times, train_corpus, bigram_threshold=50, max_bigrams=500):\n",
    "    \"\"\"\n",
    "    sentences: list of list of str -- each list is a sentence, and each str is a word\n",
    "    reading_times: list of list of float -- each list is a sentence, and each str is the reading\n",
    "        time for a word\n",
    "    train_corpus: the corpus to use to determine bigram counts for putting a threshold on bigrams\n",
    "    bigram_threshold: the minimum number of times a bigram must occur in train_corpus in order\n",
    "        to be included in the result.\n",
    "    max_bigrams: int -- the maximum number of bigrams and reading times to return\n",
    "    \n",
    "    Return two lists, one containing bigrams, and one containing reading times associated with\n",
    "    the second word in the bigram. Make sure to:\n",
    "        1. Include each bigram only once. For example, even if the bigram (\"i\", \"like\")\n",
    "           occurs 500 times in train_corpus with different reading times, it should only occur\n",
    "           once in the bigram list in the result. Randomly select which reading time to use.\n",
    "        2. Return max_bigrams bigrams or fewer. If there are more than max_bigrams in the sentences,\n",
    "           return 500 randomly selected bigrams.\n",
    "        3. Skip bigrams that occur in train_corpus fewer than bigram_threshold times.\n",
    "        4. Pad sentences with beginning of sentence and end of sentence tokens (\"<s>\" and \"<\\s>\").\n",
    "            For example in the sentence [\"i\", \"like\", toast\"], you should consider the bigrams (\"<s>\", \"i\")\n",
    "            and (\"toast\", \"<\\s>\") in addition to (\"i\", \"like\") and (\"like\", \"toast\").\n",
    "    \"\"\"\n",
    "    bigram_times = {}\n",
    "    for sentence, reading_time in zip(sentences, reading_times):\n",
    "        for index, word in enumerate(sentence):\n",
    "            if index == 0:\n",
    "                bigram = ('<s>', word.replace(\" \",\"\"))\n",
    "            elif index == len(sentence) - 1:\n",
    "                bigram = (word.replace(\" \",\"\"), '<\\s>')\n",
    "            else:\n",
    "                bigram = (word.replace(\" \",\"\"), sentence[index + 1].replace(\" \",\"\"))\n",
    "            if bigram in bigram_times:\n",
    "                bigram_times[bigram].append(reading_time[index])  \n",
    "            else:\n",
    "                bigram_times[bigram] = [reading_time[index]]\n",
    "    with open(train_corpus) as f:\n",
    "        total = f.read()\n",
    "    bigram_pool = {}\n",
    "    for key in bigram_times:\n",
    "        freq = (key[0]+\" \"+key[1])\n",
    "        if total.count(freq) >= bigram_threshold:\n",
    "            bigram_pool[key] = bigram_times[key][np.random.randint(0,len(bigram_times[key]))]\n",
    "    if len(bigram_pool) > max_bigrams:\n",
    "        keys = random.sample(list(bigram_pool.keys()), max_bigrams)\n",
    "        values = [bigram_pool[k] for k in keys]\n",
    "    else:\n",
    "        keys = list(bigram_pool.keys())\n",
    "        values = list(bigram_pool.values())\n",
    "    #printing first 10 keys and values\n",
    "    print (\"First ten bigrams: \\n\" + str(keys[0:10]))\n",
    "    print (\"and their respective reading times: \\n\" + str(values[0:10]))\n",
    "    return keys, values \n",
    "get_reading_times_and_bigrams(sentences, reading_times, \"data/bnc_10k.txt\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "Use the BNC with smoothing arpa model, and the `compute_surprisal` function defined in part a to compute the surprisal of each bigram returned by `get_reading_times_and_bigrams` in part c.\n",
    "\n",
    "Use `matplotlib` to generate a scatter plot of surprisal vs. reading time for bigrams in the Brown Corpus.\n",
    "\n",
    "Use `scipy.stats.pearsonr` to compute the linear correlation between surprisal and reading time and print the results.\n",
    "\n",
    "Based on your results, what is the relationship between surprisal and reading time? Explain why you think this is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten bigrams: \n",
      "[('have', 'a'), ('would', 'be'), ('me', 'i'), ('that', 'i'), ('able', 'to'), ('come', 'to'), ('about', 'the'), ('through', 'the'), ('that', 'he'), ('but', 'the')]\n",
      "and their respective reading times: \n",
      "[178.22, 449.89, 254.1, 419.01, 308.78, 217.04, 180.61, 218.48, 186.78, 186.83]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAHGFJREFUeJzt3X+MXeV95/H31+MBxiRh+DHJwtiO6RZNpTbamI4CXUtRFgoOJBtGaXebKNu4FZL/aLabhF039ipaNrsrxZVXTdJ/WFGcrlFYQhdcQ1tUF0GqblHNZszQOgS8uCTBM6axIxjahEkzNt/9Y841l5l77o/z457nnOfzkkYz98y5d55z557n+zzf53nOMXdHRETis67qAoiISDUUAEREIqUAICISKQUAEZFIKQCIiERKAUBEJFIKACIikVIAEBGJlAKAiEik1lddgG6uuOIK37JlS9XFEBGplaNHj/7A3Sd67Rd0ANiyZQuzs7NVF0NEpFbM7Hv97KcUkIhIpBQAREQipQAgIhKpngHAzL5qZqfN7Ftt2y4zs8fM7IXk+6XJdjOz3zWzE2b2N2Z2bdtzdiT7v2BmO8o5HBER6Vc/PYD/CXxw1bbdwOPufg3wePIY4BbgmuRrJ3AXrAQM4E7gOuB9wJ2toCEiItXoOQvI3f/CzLas2nwb8IHk5wPAnwOfS7bf6yt3mTliZuNmdmWy72Pu/gqAmT3GSlC5P/cRiNTIobkF9h0+zqnFJa4aH2PX9ilmtk5WXSyJVNZpoO9y95cB3P1lM3tnsn0SONm233yyLW27SDQOzS2w5+AxlpbPAbCwuMSeg8cAFASkEkUPAluHbd5l+9oXMNtpZrNmNnvmzJlCCydSpX2Hj5+v/FuWls+x7/DxikokscsaAL6fpHZIvp9Ots8Dm9r22wic6rJ9DXe/292n3X16YqLnQjaR2ji1uDTQdpGyZQ0AjwCtmTw7gIfbtn8ymQ10PfBakio6DNxsZpcmg783J9tEonHV+NhA20XK1s800PuBvwKmzGzezG4H9gI3mdkLwE3JY4BHgReBE8DvAb8BkAz+/lfgm8nXf2kNCIvEYtf2KcZGR96ybWx0hF3bpyoqkcTOVibshGl6etp1LSBpEs0CkmEws6PuPt1rv6AvBifSNDNbJ1XhSzB0KQgRkUgpAIiIREoBQEQkUgoAIiKRUgAQEYmUAoCISKQUAEREIqUAICISKQUAEZFIKQCIiERKAUBEJFIKACIikVIAEBGJlAKAiEikFABERCKlACAiEikFABGRSCkAiIhESgFARCRSCgAiIpFSABARiZQCgIhIpBQAREQipQAgIhIpBQARkUgpAIiIREoBQEQkUgoAIiKRUgAQEYmUAoCISKQUAEREIqUAICISKQUAEZFIKQCIiEQqVwAws8+a2bNm9i0zu9/MLjKzq83sKTN7wcweMLMLkn0vTB6fSH6/pYgDEBGRbDIHADObBP4dMO3uPweMAB8Dfhv4krtfA7wK3J485XbgVXf/aeBLyX4iIlKRvCmg9cCYma0HNgAvAzcADya/PwDMJD/fljwm+f2NZmY5/76IiGSUOQC4+wLw34GXWKn4XwOOAovufjbZbR6YTH6eBE4mzz2b7H/56tc1s51mNmtms2fOnMlaPBER6SFPCuhSVlr1VwNXARcDt3TY1VtP6fK7Nze43+3u0+4+PTExkbV4IiLSQ54U0C8C33H3M+6+DBwE/jkwnqSEADYCp5Kf54FNAMnvLwFeyfH3RUQkhzwB4CXgejPbkOTybwS+DXwD+OVknx3Aw8nPjySPSX7/hLuv6QGIiMhw5BkDeIqVwdyngWPJa90NfA64w8xOsJLj3588ZT9webL9DmB3jnKLiEhOFnIjfHp62mdnZ6suhohIrZjZUXef7rWfVgKLiERKAUBEJFIKACIikVIAEBGJlAKAiEikFABERCKlACAiEikFABGRSCkAiIhEan3vXSRkh+YW2Hf4OKcWl7hqfIxd26eY2TrZ+4kiEj0FgBo7NLfAnoPHWFo+B8DC4hJ7Dh4DUBAQkZ6UAqqxfYePn6/8W5aWz7Hv8PGKSiQidaIAUGOnFpcG2i4i0k4BoMauGh8baLuISDsFgBrbtX2KsdGRt2wbGx1h1/apikokInWiQeAaaw30ahaQiGShAFBzM1snVeGLSCZKAYmIREoBQEQkUgoAIiKRUgAQEYmUAoCISKQUAEREIqUAICISKQUAEZFIKQCIiERKAUBEJFIKACIikVIAEBGJlAKAiEikFABERCKlACAiEikFABGRSCkAiIhEKlcAMLNxM3vQzJ43s+fM7BfM7DIze8zMXki+X5rsa2b2u2Z2wsz+xsyuLeYQREQki7w9gK8Af+ruPwP8M+A5YDfwuLtfAzyePAa4Bbgm+doJ3JXzb4uISA6ZA4CZvQN4P7AfwN1/4u6LwG3AgWS3A8BM8vNtwL2+4ggwbmZXZi65iIjkkqcH8FPAGeD3zWzOzO4xs4uBd7n7ywDJ93cm+08CJ9ueP59sExGRCuQJAOuBa4G73H0r8CPeTPd0Yh22+ZqdzHaa2ayZzZ45cyZH8UREpJs8AWAemHf3p5LHD7ISEL7fSu0k30+37b+p7fkbgVOrX9Td73b3aXefnpiYyFE8ERHpJnMAcPe/A06a2VSy6Ubg28AjwI5k2w7g4eTnR4BPJrOBrgdea6WKRERk+NbnfP5vAveZ2QXAi8CvsxJU/sDMbgdeAv5Vsu+jwK3ACeD1ZF8REalIrgDg7s8A0x1+dWOHfR34VJ6/JyIixdFKYBGRSCkAiIhESgFARCRSCgAiIpHKOwtIhENzC+w7fJxTi0tcNT7Gru1TzGzVIm+R0CkASC6H5hbYc/AYS8vnAFhYXGLPwWMACgIigVMKSHLZd/j4+cq/ZWn5HPsOH6+oRCLSLwUAyeXU4tJA20UkHAoAkstV42MDbReRcCgASC67tk8xNjrylm1joyPs2j6V8gwRCYUGgSWX1kCvZgGJ1I8CgOQ2s3VSFb5IDSkFJCISKQUAEZFIKQCIiERKAUBEJFIKACIikVIAEBGJlAKAiEiktA5ApOF0uW5JowAg0oe6VqK6XLd0oxSQSA+tSnRhcQnnzUr00NxC1UXrSZfrlm7UA6iJurZAm6BbJRr6/0CX65Zu1AOogTq3QJugzpWoLtct3SgA1ECebvyhuQW27X2Cq3f/Cdv2PqGgkUGdK1Fdrlu6UQCogYWMLdAQeg5NCEB1rkRntk7yxY++h8nxMQyYHB/jix99T/CpKxkOjQEE7tDcAgZ4h9/1aoFWnbtuygyUut/zQJfrljQKAIHbd/h4x8rfoGcLtOrcddUBqEiqRKWJlAIKXFpl7fRuRVedu646AIlIdwoAgUurrCf7qMSrzl0PKwA1YZxBpAoKAIHLU4lXPQA4jAAUwkC3SF1pDKAAZS7SyjsAWWXuehiDp00aZ+hECwClTAoAOQ1jpkudByDLLnuTxxmaMotKwqUUUE661kq1qh7oLpM+W1I2BYCcmtwCrYOqB7qL1j6gnXUBoEi/cgcAMxsxszkz++Pk8dVm9pSZvWBmD5jZBcn2C5PHJ5Lfb8n7t0PQ5BZoHVQ90F2k1QPaafTZkqIUMQbwaeA54B3J498GvuTuXzez/wHcDtyVfH/V3X/azD6W7PcrBfz9Su3aPvWWPC3UuwVaR3UeI2nXKeWzWtpnS4PFkkWuHoCZbQQ+BNyTPDbgBuDBZJcDwEzy823JY5Lf35jsX2tVtEA1772ZuqV2un22Ok2F/ewDz/D5Q8fKLbDUXt4ewJeB3wLenjy+HFh097PJ43mg9WmdBE4CuPtZM3st2f8H7S9oZjuBnQCbN2/OWbzhGGYLVDNDmuuq8bGOef/J8TGe3H1D6vM69RwcuO/IS0y/+zJ9LiRV5h6AmX0YOO3uR9s3d9jV+/jdmxvc73b3aXefnpiYyFq8xtLMkObKOqDd7XIh+lxIN3l6ANuAj5jZrcBFrIwBfBkYN7P1SS9gI3Aq2X8e2ATMm9l64BLglRx/P0qaddRcWRfOpfUcQJ8L6S5zAHD3PcAeADP7APAf3P0TZva/gV8Gvg7sAB5OnvJI8vivkt8/4e7dJjuUou6DZWkn++qZIXU/TmjGMQwqSzpx1/YpPvvAM5kuGS5xK2MdwOeAO8zsBCs5/v3J9v3A5cn2O4DdJfztrppw3Zh+0gRNOM4mHMOwzGyd5BPXb16TY9VsNOnFKmiE9216etpnZ2cLe71te5/INMgWml4t47TjHB8b5Zk7bx5mUTNryv+qH0X1dGLsMUlnZnbU3ad77RfVtYCakj/vliY4NLeQmg9eXFrm0NxCLSqFQf9Xda38ipzV1ZT1EDI8UV0KoumrdluVSTehzwpprXFI65d2+l/VOV2kWV1SpagCQNOuG7NaPytJQ+7ttFfknaT9r+pciTalVyr1FFUKqO439+6ln0oj5N5OtwA22eV/VedKtN9ZXSJliCoAQLPzpN3mg0M1vZ1BcvNpFbZB14HfOleiupaUVCmqFFDTdUpxtaYGVnGVzEFz81nHaOqc2qvqaqa6npRAhD2AJgstxTXo7RqztoZDO+5BDbtXqutJSYsCQMOElOIaNDefpyIP6bhD1/T7KEv/FAAarqz58f28bpbcfJkVeV3XChRtGIPmeq/rQQGgwcrq6vf7ut1SOsOuINLKPPu9V/jG82dqWVFlfQ/LHjRXiqk+NAjcYGXNj+/3ddMGOIGhL9xKK/N9R16q5QKyPIvfyh40r/O6jNg0MgBohsOKsrr6eV+3igqi2zXzh1mOouR5D8ueeVTndRmxaVwKKJbuZ1k5+H4McknqTv+LtMVeZVYQvdZIDKscRclbyZY51jLsdRkab8iucT2AGLqf/Xb/y+jqH5pb4Ef/eHbN9k6vm/a/GEm5FXSRFcTnDx3jn+55lC27/2Tl++VjqWskyixHWUK+rtUw12XU+TpQIWhcAIih+5k3B5+1ddQ62RaXlt+y/dINox1fN+09P+deagXx+UPH+NqRlziXXOr8nDtP/u0rXLv5kre8F5+4fnNtF5CFvPhtmIvbmtbgG3b6unEpoDpfFqBfaRXrwuIS2/Y+saYrXNSJl3atng0XrO/4N7rd5HzX9qnSuu33P3Wy4/YjL77K337x1rdsm373ZYWVY5ipiCoXv/VznMNal9GkBl8V6evGBYAYrq2SVrEanN9exodn0JOt2/+izAriXMpNjjptL6ocVZy8VSx+6/c4VweJf/EzE6VMt21Sg6+KBXqNSwFVdW2VYUq75k8ZM1rau6TrBszdV/W/SBtjSNtehJBTEUWmFfo5zk55+a+VNN025FTYoKrozTSuBwDNvyxAp+5/2gyXPB+e1a29Ti3oXidb2f+LTumIj1+3ia8deWnNvh+/blNp5Qg1FZG1Z5KW5unnOPu5L0VRLdu6XweqXRW9mUYGgBisrljT7qGb58OTdiKPmPGGe+UnW1rl1lpsdv9TJznnzogZH79uE/9t5j2llSXUVESWtEK3oNHPcfYb9IoKjkU0MkKYSlpF+loBoCHK+PCknaBvuPOdvR/K/LrdDJI77la5Pbn7hkIr/F4VRBUnbz+VVpaeSbf3tZ/j7HfNRdXBsSWUtUNV9GYUABrkwvXrzn+IL90wyp3/8mdzfXjKbNW2V17jG0ZxX7lpfftYRit3TNvj9hNzWGmXfiqIYZ+8/VZaWf6H3d7Xfo6zU5BYLaQ8fUhXRx12+loBoAFWVwYAP15+I/frltWqXV3eV19/c11B2s3gW9pPzGGlXfqtIIZ58vZbpiz/w17va6/j7BQkypoFVIRQx2+GQQGgAcpqwZTVqu1nkLCb1ok5rLRLiBVEtzKtTg390s9PDlT5FvG+1mkiRqjjN8OgANAAZVZQZZzIecvV3hKF4gPU6gr0krHRNauf28tRtk65/rRKa3zD6JrU0ENHFwaaftukmTX9iGHtUJqoA0AII/9FqFsLZpALs622+sQsOkB1yq2PjhjrgPak2ug6G0oFkZbr/6Wfn+ShowtrKi13CukN1qkFv9qg53VsAa9dtAEglJH/ItStBdNrkLA1EDxZQe64U3pq+VyHkYkC15R1q7DS0nvfeP4MX/zoe/jCHz17fgzlwvXrOvZUoL9eVxMaRFnP6zoHvDyiDQAhjfznFVoLpldFsrq8rVlAry0tV172ftNTy+f8/OrXPO97rwqrV3qvfbB/9Syqdr16g01pEDXpvB6GaANAiAN7eYTSgum3IgmlvKsNkp5afX+DLJVmrworrTyXjI12fK6z9rIg/fQGQ6k48/ZCQjyvQ+5ZNe5aQP0K+XrqdRbyNXH6kXadpU5GzHIfa68Ka9f2KUbXrS3Bj35yNjVQtdJnq6+/1O2aQCFUnEVc2z+08zr0+xVEGwBCvohUnW9pGUJFkkenC9il3Tcg7aqjgxxrrwprZuskb7tobUd9+ZynXtxucnyMJ3ffwHf2fognd99wvvJfXRF95oFneO8X/oxDcws9yzGMz2QRjYfQzuvQG0TRpoBCyJt36hoCpeZiy+6O1m1GUied0lOd7huw7/Dx3MfazwD+4uudB3ZbN9bpZ/A/be3F4tJy11lFu7ZPDW18oIjGQwjndbvQG0TRBgCoNg+ddlJdNLqutFzsME7ksmckVZVPTfusFLFgCrpXWEXcWKdbhdM+q6jTa23b+8RQxgeKajyENL4UeoMo6gBQpbSuYZk3TB/GQF+ZLbDQZqoUday9KqwibqzTa3C7dZ2fQYJH0a3Yuk1n7kfox5Q5AJjZJuBe4J+wskbmbnf/ipldBjwAbAG+C/xrd3/VzAz4CnAr8Drwa+7+dL7i19egJ08RLYZBbyWZVVktsLQA9oU/erayFt8wWptFBJpeay+6fb6G1YoNLX1ThNCPKU8P4Czw7939aTN7O3DUzB4Dfg143N33mtluYDfwOeAW4Jrk6zrgruR7lFKX8o+N8o9n3yilxVDVrSSLkhbAXn19mUNzC8GVt0h5A03rue0Lx1p6fb6G2YoNKX1TlJCPKfMsIHd/udWCd/d/AJ4DJoHbgAPJbgeAmeTn24B7fcURYNzMrsxc8ppLm63wnz/ys6XdRnGYt5IsQ7cWZ4jlDc3M1knm/tPNfPlX3jvQ56vTzKgyb+1Z51lwdVPIGICZbQG2Ak8B73L3l2ElSJjZO5PdJoGTbU+bT7a9XEQZ6qZX17CMk6vT3yzjVpJF6XRzmE63eoQwyttLKAuCsrRIh9WKDW2cp+lyBwAzexvwEPAZd/97S7/xdqdfrJlIbWY7gZ0Amzdvzlu8oFXRNVz9N8u4lWQROlUEDx1dYMPoOl7vcK+DqsvbS5aKLZSAMUyhrEiORa6FYGY2ykrlf5+7H0w2f7+V2km+n062zwPtd+XeCJxa/Zrufre7T7v79MTERJ7iSR+yLJypclHQBetHOqaxWgPZoaYLBl0QFPoK0rKEPm++aTIHgGRWz37gOXf/nbZfPQLsSH7eATzctv2TtuJ64LVWqkiqM2h+d1gVU9oJ/9rS8vnyAmtuIRlqJTloxRb6CtKyhHYph6bL0wPYBvwqcIOZPZN83QrsBW4ysxeAm5LHAI8CLwIngN8DfiPH35YCzWydXHPpgDTDqpi6VQSt8k6Oj9V+ADtte6wt4dAu5dB0mccA3P0vSb9O1o0d9nfgU1n/ngxXWv45pEVBdaokB51KGfoK0rKEPm++abQSWNboNmAZ0qKgOlWSg1Zsoa8gLVPI8+abxjzlioYhmJ6e9tnZ2aqLEZ20mUGta890qpjKnBeeZnWgqrIsZQh1FlCo5ZI3mdlRd5/utZ96ALJGt9RKSF30QctSt4orxJZwnnn6dXv/Y6AAIGv0Sq2EVDH1WxYtMCpG1nn6w3r/FWQGE+0NYWRFpzn9TZyJEeu0yqJlHXgfxvsf69qJPBQAIpZ2wgBDvfbLMNRpxlDI0gbYxzeMdn3eMN7/ooJMTNciUgooYt1OmF7rAeqmTjOGilZkWmTX9il2PfjXLJ976+SRH/74bNcrsg7j/S8iyMSWKlQPIGJ1bhUP2kprYlqrH0WnRWa2TnLxBR3uUfyGd21pD+P9L2IVcWypQvUAIlbXVnGWVlpIs5fK1t7iX2e25ub1eS+u9tpS53sUd2s4DOP9L2LtRJ0bRVkoAESsrouNss5ECWn2UllWB8fVlX9Lngota8Oh7Pe/iCBT10ZRVgoAEevnhAlxWl1srbRBdAqOneSp0EJuOOQNMiEfWxkUACLX7YQJdUAstlbaIPoJgnkrtCan05p8bJ0oAEiqUG/OEVsrbRBpwXHEjDfcC6vQmpxOa/KxraYAIKlCTbV0usH5hes1oQ3Sg2Pd13FIORQAAhVC7j30VMuP224Nubi0HER6qmqxpTAkHwWAAIWSew851RJqeioEMaUwJB8FgACFUrmF3JocJD0VQm9KJEQKAAEKKfceamuy3/RUKL2pYVCgk0Fp5CxAujF2b/1eWiCWpf26EqZkoQAQoFivWzOIma2TfV2xNKTeVJliCXRSLKWAAhRy7j0k/aSnQp/JVJRYAp0USwEgUKHm3usm5JlMRYol0EmxlAKSRus3VVR3ShtKFuoBSOPF0JtS2lCyUAAQaYgYAp0USykgEZFIKQCIiERKAUBEJFIKACIikVIAEBGJlHnKTaNDYGZngO/1ufsVwA9KLE7Z6l5+0DGEoO7lh/ofQwjlf7e7T/TaKegAMAgzm3X36arLkVXdyw86hhDUvfxQ/2OoU/mVAhIRiZQCgIhIpJoUAO6uugA51b38oGMIQd3LD/U/htqUvzFjACIiMpgm9QBERGQAtQ8AZvZBMztuZifMbHfV5RmUmX3VzE6b2beqLksWZrbJzL5hZs+Z2bNm9umqyzQoM7vIzP6vmf11cgxfqLpMWZjZiJnNmdkfV12WLMzsu2Z2zMyeMbPZqsuThZmNm9mDZvZ8ck78QtVl6qbWKSAzGwH+H3ATMA98E/i4u3+70oINwMzeD/wQuNfdf67q8gzKzK4ErnT3p83s7cBRYKZm/wMDLnb3H5rZKPCXwKfd/UjFRRuImd0BTAPvcPcPV12eQZnZd4Fpd696Dn1mZnYA+D/ufo+ZXQBscPfFqsuVpu49gPcBJ9z9RXf/CfB14LaKyzQQd/8L4JWqy5GVu7/s7k8nP/8D8BxQq2sS+4ofJg9Hk69atYzMbCPwIeCeqssSKzN7B/B+YD+Au/8k5Mof6h8AJoGTbY/nqVnl0yRmtgXYCjxVbUkGl6RPngFOA4+5e92O4cvAbwFvVF2QHBz4MzM7amY7qy5MBj8FnAF+P0nF3WNmF1ddqG7qHgCsw7ZatdyawszeBjwEfMbd/77q8gzK3c+5+3uBjcD7zKw26Tgz+zBw2t2PVl2WnLa5+7XALcCnkvRonawHrgXucvetwI+AoMcl6x4A5oFNbY83AqcqKku0krz5Q8B97n6w6vLkkXTZ/xz4YMVFGcQ24CNJDv3rwA1m9rVqizQ4dz+VfD8N/CErKd46mQfm23qPD7ISEIJV9wDwTeAaM7s6GXD5GPBIxWWKSjKAuh94zt1/p+ryZGFmE2Y2nvw8Bvwi8Hy1peqfu+9x943uvoWVc+AJd/83FRdrIGZ2cTKJgCRtcjNQq5lx7v53wEkzm0o23QgEPRmi1vcEdvezZvZvgcPACPBVd3+24mINxMzuBz4AXGFm88Cd7r6/2lINZBvwq8CxJIcO8B/d/dEKyzSoK4EDyayydcAfuHstp1LW2LuAP1xpT7Ae+F/u/qfVFimT3wTuSxqkLwK/XnF5uqr1NFAREcmu7ikgERHJSAFARCRSCgAiIpFSABARiZQCgIhIpBQAREQipQAgIhIpBQARkUj9f8coUsuCcGqCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.1885538574973634, 0.05646817155226596)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as lin\n",
    "\n",
    "def generate_scatter_plot(model, bigrams, reading_times):\n",
    "    surprisals = []\n",
    "    for bigram in bigrams:\n",
    "        b_conversion = bigram[0] + \" \" + bigram[1]\n",
    "        surprisals.append(compute_surprisal(model, b_conversion))\n",
    "    plt.scatter(surprisals, reading_times)\n",
    "    plt.show()\n",
    "    linear_correlation = lin.pearsonr(surprisals, reading_times)\n",
    "    print (linear_correlation)\n",
    "    \n",
    "bigrams, times = get_reading_times_and_bigrams(sentences, reading_times, \"data/bnc_10k.txt\")\n",
    "generate_scatter_plot(model_with_smoothing, bigrams, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient for surprisals vs. reading time is closer to 0 than -1 or 1. That means there is no correlation between surprisal and reading time. The reason might be due to the smoothing done to the data. To further investigate this, we generate the function again but using model without smoothing to test this hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "\n",
    "Using the same bigrams returned by `get_reading_times_and_bigrams` from part c, compute surprisal based on the BNC model **without** smoothing.\n",
    "\n",
    "Generate a scatter plot and compute the linear correlation for the results without smoothing, as you did in part d. Do your results differ from the results in part d? If so, explain why, and if not, explain why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.1894470455038596, 0.05528784333855638)\n"
     ]
    }
   ],
   "source": [
    "generate_scatter_plot(model_without_smoothing, bigrams, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can say that given that the correlation coefficient is near 0, as it was in the last computation, the data, with or without smoothing, does not show correlation with reading time. This means that smoothing has no effect on the outcome of these experiments, and further strengthens that there is no relationship between surprisal and reading time. This may allow basis for an argument that surprisal is not related to reading time, and perhaps even that surprisal is not related to semantics at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Surprisal and Concreteness\n",
    "\n",
    "Given the experiments we've done so far, we don't really know if surprisal is connected to semantics. One prominent element of semantics that cognitive scientists have found plays a role in word processing is concreteness. In the next part of the problem, we'll look at the relationship between surprisal and concreteness. To do this, we'll use the Calgary Concreteness data set$^4$, which has several English nouns and their concreteness ratings on a scale from 1 to 5. This dataset is in `data/calgary_concreteness.csv`.  We provide the functions that read in info from this dataset in `provided_functions.py`. Using this data, we can compare the average surprisal of words that are relatively high in concreteness to those that are relatively low in concreteness, to see how concreteness affects reading times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)\n",
    "\n",
    "Write a function `compute_average_word_surprisal`, which computes the average surprisal for a given word based on `max_bigrams` randomly selected contexts that that word occurs in in the BNC corpus.\n",
    "\n",
    "Use the function `compute_average_word_surprisal` to find the average surprisal for the word *cat*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from provided_functions import get_bigram_counts\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def compute_average_word_surprisal(word, bigram_counter, model, max_bigrams=10):\n",
    "    \"\"\"\n",
    "    word: string -- the word to compute average surprisal for\n",
    "    bigram_counter: collections.Counter -- a Counter mapping bigram tuples to frequency in a corpus.\n",
    "                    This can be comptued using the function get_bigram_counts in provided_functions.py\n",
    "    model: arpa model -- an arpa model\n",
    "    max_bigrams: int -- number of sampled bigrams used to compute the average surprisal of word\n",
    "    \n",
    "    Return the average surprisal of a bigram containing word in index 1. For example,\n",
    "    for the word 'cat', we would consider ('my', 'cat') but not ('cat', 'went'). In cases where there\n",
    "    are more than max_bigrams in bigram_counter that contain word in index 0, compute average surprisal\n",
    "    based on max_bigrams randomly selected bigrams.\n",
    "    \"\"\"\n",
    "    bigram_pool = []\n",
    "    average = 0\n",
    "    for bigram in bigram_counter:\n",
    "        if bigram[1] == word:\n",
    "            bigram_pool.append(compute_surprisal(model, (bigram[0]+\" \"+bigram[1])))\n",
    "    if len(bigram_pool) > max_bigrams:\n",
    "        bigram_pool = random.sample(bigram_pool, max_bigrams)\n",
    "    total = sum(bigram_pool)\n",
    "    average = total/len(bigram_pool)\n",
    "    return average\n",
    "\n",
    "bigram_counts = get_bigram_counts('data/bnc_10k.txt')\n",
    "model_with_smoothing = arpa.loadf(\"data/bnc_10k_with_smoothing.arpa\")[0]\n",
    "compute_average_word_surprisal('cat', bigram_counts, model_with_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g)\n",
    "\n",
    "Take a look at the files `data/low_concreteness_nouns.txt` and `data/high_concreteness_nouns.txt`. These files each contain a subset of 100 nouns from the Calgary Concreteness data set with low concreteness (< 2.5), high concreteness (> 3.5) respectively. They are the most common nouns of each type based on frequency in the BNC sample.\n",
    "\n",
    "Use `matplotlib` to generate a scatter plot of surprisal vs. concreteness for the low concreteness nouns in `data/low_concreteness_nouns.txt`. You should use `compute_average_word_surprisal` from part f as well as the functions `load_words`, `get_bigram_counts`, and `read_concreteness` defined in `provided_functions.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "become\n",
      "also\n",
      "must\n",
      "seem\n",
      "act\n",
      "power\n",
      "including\n",
      "offer\n",
      "interest\n",
      "describe\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from provided_functions import read_concreteness, load_words, get_bigram_counts\n",
    "\n",
    "#This takes a very long time to compute. However, the code works. \n",
    "#I had to shorten the txt file in order to run this thing within a reasonable time\n",
    "def generate_scatter_concrete(concreteness, words, counts, nouns):\n",
    "    surprisals = []\n",
    "    concrete = []\n",
    "    for noun in nouns:\n",
    "        print (noun)\n",
    "        surprisals.append(compute_average_word_surprisal(noun, counts, model_with_smoothing, 10))\n",
    "        concrete_index = words.index(noun)\n",
    "        concrete.append(concreteness[concrete_index])\n",
    "    plt.scatter(surprisals, concrete)\n",
    "    plt.show()\n",
    "\n",
    "bigram_counts = get_bigram_counts('data/bnc_10k.txt')                              \n",
    "concreteness, words = read_concreteness(\"data/calgary_concreteness.csv\")\n",
    "nouns = load_words(\"data/low_concreteness_nouns.txt\")       \n",
    "generate_scatter_concrete(concreteness, words, bigram_counts, nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (h)\n",
    "\n",
    "Use the same approach that you used in part g to generate a scatter plot of surprisal vs. concreteness the nouns in `data/high_concreteness_nouns.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-3220ede3da03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnouns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/high_concreteness_nouns.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerate_scatter_concrete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcreteness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnouns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-189-f6441cde9696>\u001b[0m in \u001b[0;36mgenerate_scatter_concrete\u001b[0;34m(concreteness, words, counts, nouns)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnoun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnouns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnoun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msurprisals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_average_word_surprisal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_with_smoothing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mconcrete_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcreteness\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconcrete_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-84935b1fa19f>\u001b[0m in \u001b[0;36mcompute_average_word_surprisal\u001b[0;34m(word, bigram_counter, model, max_bigrams)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbigram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbigram_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mbigram_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_surprisal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_pool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_bigrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mbigram_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-97dd830e591c>\u001b[0m in \u001b[0;36mcompute_surprisal\u001b[0;34m(arpa_model, bigram)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msurprisal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marpa_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msurprisal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/arpa/models/base.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(self, ngram)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/arpa/models/base.py\u001b[0m in \u001b[0;36mlog_p\u001b[0;34m(self, ngram)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_unks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_p_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/arpa/models/base.py\u001b[0m in \u001b[0;36m_replace_unks\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_replace_unks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/arpa/models/base.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_replace_unks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/arpa/models/base.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/arpa/models/simple.py\u001b[0m in \u001b[0;36mvocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/arpa/models/simple.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nouns = load_words(\"data/high_concreteness_nouns.txt\")       \n",
    "generate_scatter_concrete(concreteness, words, bigram_counts, nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i)\n",
    "\n",
    "What is the relationship between surprisal and concreteness? Is the relationship different for high and low concreteness nouns? Explain why you think this is or is not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between surprisal and concreteness is linear.\n",
    "\n",
    "The relationship is inverted between high and low concreteness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "$^1$Smith, N. J., & Levy, R. (2013). The effect of word predictability on reading time is logarithmic. Cognition, 128(3), 302-319. doi:10.1016/j.cognition.2013.02.013\n",
    "\n",
    "$^2$The British National Corpus, version 3 (BNC XML Edition). 2007. Distributed by Bodleian Libraries, University of Oxford, on behalf of the BNC Consortium. URL: http://www.natcorp.ox.ac.uk/\n",
    "\n",
    "$^3$A Standard Corpus of Present-Day Edited American English, for use with Digital Computers (Brown). 1964, 1971, 1979. Compiled by W. N. Francis and H. Kučera. Brown University. Providence, Rhode Island.\n",
    "\n",
    "$^4$Pexman, P. M., Heard, A., Lloyd, E., & Yap, M. J. (2016). The Calgary semantic decision project: Concrete/abstract decision data for 10,000 English words. Behavior Research Methods, 49(2), 407-417. doi:10.3758/s13428-016-0720-6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
